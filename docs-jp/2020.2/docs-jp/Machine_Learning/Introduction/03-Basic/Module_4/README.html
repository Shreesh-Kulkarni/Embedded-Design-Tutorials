


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		
		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="../../../../_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="../../../../_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

  
  
  
  

  
      <script type="text/javascript" src="../../../../_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="../../../../_static/js/gtm.js"></script>
  <script type="text/javascript" src="../../../../_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="../../../../_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="../../../../_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="../../../../_static/js/header-footer.min.js"></script>
    <script type="text/javascript" src="../../../../_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="../../../../_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="../../../../_static/js/linkid.js"></script>
    <script type="text/javascript" src="../../../../_static/js/Searchbox.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/pro.min.css" media="all" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="header parbase aem-GridColumn aem-GridColumn--default--12">
								<noindex>
									<header data-component="header">
										<nav class="navbar navbar-default aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid main-nav">
													<div class="row">
														<div class="col-xs-12">
															<div class="logo-column">
																<div class="logo">
																	<a href="https://www.xilinx.com/">
																	<img src="https://www.xilinx.com/etc.clientlibs/site/clientlibs/xilinx/all/resources/imgs/header/xilinx-header-logo.svg" title="Xilinx Inc"/>
																	</a>
																</div>
															</div>
															<div class="navbar-column">
																<div class="navbar navbar-collapse collapse" id="xilinx-main-menu">
																	<div class="mobile-search-container">
																		<div id="headerSearchBox" class="headerSearch"
																			data-component="header-search"
																			data-redirect-if-empty="false"
																			data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																			data-coveo-organization-id="xilinxcomprode2rjoqok">
																			<div class='coveo-search-section'>
																				<div class="CoveoAnalytics" data-search-hub="Site"></div>
																				<ul class="dropdown-menu options">
																					<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																						<a href="#">
																						All</a>
																					</li>
																					<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com//products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Silicon Devices</a>
																					</li>
																					<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com//products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Boards and Kits</a>
																					</li>
																					<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com//products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Intellectual Property</a>
																					</li>
																					<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																						<a href="#">
																						Support</a>
																						<ul>
																							<li data-label="Documentation" data-action-link="https://www.xilinx.com//support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																								<a href="#">
																								Documentation</a>
																							</li>
																							<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com//support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																								<a href="#">
																								Knowledge Base</a>
																							</li>
																							<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																								<a href="#">
																								Community Forums</a>
																							</li>
																						</ul>
																					</li>
																					<li data-label="Partners" data-action-link="https://www.xilinx.com//alliance/member-keyword-search.html" data-search-hub="Partner">
																						<a href="#">
																						Partners</a>
																					</li>
																					<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																						<a href="#">
																						Videos</a>
																					</li>
																					<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																						<a href="#">
																						Press</a>
																					</li>
																				</ul>
																				<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																				<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																			</div>
																		</div>
																	</div>
																	<ul class="nav navbar-nav nav-justified">
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/applications.html">
																			Applications</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/products/silicon-devices.html">
																			Products</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://developer.xilinx.com/">
																			Developers</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/support.html">
																			Support</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/about/company-overview.html">
																			About</a>
																		</li>
																	</ul>
																</div>
															</div>
															<script type="text/javascript" src="../../../../_static/js/gtm.js"></script>
															<!--<div class="mini-nav">
																<button type="button" data-function="xilinx-mobile-menu" id="nav-toggle" class="navbar-toggle collapsed visible-xs-block" aria-expanded="false">
																<span></span>
																<span></span>
																<span></span>
																<span></span>
																</button>
																<ul class="list-inline">
																	<li class="dropdown user-menu">
																		<button data-toggle="dropdown">
																		<span class="sr-only">Account</span>
																		<span class="fas fa-user"></span>
																		</button>
																		<ul class="dropdown-menu">
																			<li>
																				<a href="https://www.xilinx.com/myprofile/subscriptions.html">
																				My Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/registration/create-account.html">
																				Create Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/bin/protected/en/signout">
																				Sign Out</a>
																			</li>
																		</ul>
																	</li>
																	<li class="hidden-xs">
																		<button data-function="search-toggle">
																		<span class="sr-only">Search</span>
																		<span class="far fa-search"></span>
																		</button>
																	</li>
																</ul>
															</div>
															-->
															<div class="search-container">
																<div id="headerSearchBox" class="headerSearch"
																	data-component="header-search"
																	data-redirect-if-empty="false"
																	data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																	data-coveo-organization-id="xilinxcomprode2rjoqok">
																	<div class='coveo-search-section'>
																		<div class="CoveoAnalytics" data-search-hub="Site"></div>
																		<ul class="dropdown-menu options">
																			<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																				<a href="#">
																				All</a>
																			</li>
																			<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com/products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Silicon Devices</a>
																			</li>
																			<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com/products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Boards and Kits</a>
																			</li>
																			<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com/products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Intellectual Property</a>
																			</li>
																			<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																				<a href="#">
																				Support</a>
																				<ul>
																					<li data-label="Documentation" data-action-link="https://www.xilinx.com/support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																						<a href="#">
																						Documentation</a>
																					</li>
																					<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com/support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																						<a href="#">
																						Knowledge Base</a>
																					</li>
																					<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																						<a href="#">
																						Community Forums</a>
																					</li>
																				</ul>
																			</li>
																			<li data-label="Partners" data-action-link="https://www.xilinx.com/alliance/member-keyword-search.html" data-search-hub="Partner">
																				<a href="#">
																				Partners</a>
																			</li>
																			<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																				<a href="#">
																				Videos</a>
																			</li>
																			<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																				<a href="#">
																				Press</a>
																			</li>
																		</ul>
																		<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																		<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																	</div>
																</div>
																<button data-function="search-toggle">
																<span class="sr-only">Search</span>
																<span class="far fa-times"></span>
																</button>
															</div>
														</div>
													</div>
												</div>
											</div>
										</nav>
									</header>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> Vitis チュートリアル
          

          
          </a>

          
            
            
              <div class="version">
                2020.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <p class="caption"><span class="caption-text">English version</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs/index.html">Master</a></li>
</ul>
<p class="caption"><span class="caption-text">入門</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Getting_Started/Vitis/README.html">Vitis フロー 101 チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Getting_Started/Vitis_HLS/README.html">Vitis HLS の解析および最適化</a></li>
</ul>
<p class="caption"><span class="caption-text">機械学習 (英語版)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../README.html">Introduction to Machine Learning with Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">アクセラレーション</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Hardware_Accelerators/README.html">Vitis ハードウェア アクセラレータの概要</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Hardware_Accelerators/README.html#id1">設計チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Hardware_Accelerators/README.html#id2">機能チュートリアル</a></li>
</ul>
<p class="caption"><span class="caption-text">AI エンジン開発 (英語版)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../AI_Engine_Development/README.html">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../AI_Engine_Development/README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">プラットフォーム作成チュートリアル</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Vitis_Platform_Creation/README.html">プラットフォームの作成</a></li>
</ul>
<p class="caption"><span class="caption-text">XRT および Vitis システム最適化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../Runtime_and_System_Optimization/README.html">設計チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../Runtime_and_System_Optimization/README.html#id2">機能チュートリアル</a></li>
</ul>
<p class="caption"><span class="caption-text">バージョン</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-1/docs/README.html">2020.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-Tutorials/blob/Vitis-Tutorials-2019.2-Hotfix1/README.md">2019.2</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="../../../../_sources/Machine_Learning/Introduction/03-Basic/Module_4/README.md.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Vitis チュートリアル</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>3.4 CIFAR10 Classification using Vitis AI and TensorFlow</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../../_sources/Machine_Learning/Introduction/03-Basic/Module_4/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="cifar10-classification-using-vitis-ai-and-tensorflow">
<h1>3.4 CIFAR10 Classification using Vitis AI and TensorFlow<a class="headerlink" href="#cifar10-classification-using-vitis-ai-and-tensorflow" title="Permalink to this headline">¶</a></h1>
<p>This module introduces the Vitis AI TensorFlow design process and illustrates how to go from a Python description of the network model to running a compiled model on the Xilinx® DPU accelerator. Please visit the following link for more <a class="reference external" href="https://gitenterprise.xilinx.com/swm/Vitis-In-Depth-Tutorial/tree/v1.3/Machine_Learning/Design_Tutorials">Design Tutorials</a></p>
<p>The application code in this example design is written in Python and uses the Unified APIs and VART runtime that were introduced in Vitis™ AI v1.3.</p>
<p>We will run the following steps:</p>
<ol class="simple">
<li><p>Training and evaluation of a customized version of the DenseNet network using TensorFlow Keras.</p></li>
<li><p>Conversion of the HDF5 format Keras checkpoint into a TensorFlow compatible checkpoint.</p></li>
<li><p>Removal of the training nodes and conversion of the graph variables to constants (..often referred to as ‘freezing the graph’).</p></li>
<li><p>Evaluation of the frozen model using the CIFAR-10 test dataset.</p></li>
<li><p>Quantization of the floating-point frozen model.</p></li>
<li><p>Evaluation of the quantized model using the CIFAR-10 test dataset.</p></li>
<li><p>Compilation of the quantized model to create the <code class="docutils literal notranslate"><span class="pre">.xmodel</span></code> file ready for execution on the DPU accelerator IP.</p></li>
<li><p>Download and execution of the application on an evaluation board.</p></li>
</ol>
</div>
<div class="section" id="the-cifar-10-dataset">
<h1>The CIFAR-10 Dataset<a class="headerlink" href="#the-cifar-10-dataset" title="Permalink to this headline">¶</a></h1>
<p>CIFAR-10 is a publicly available dataset that contains a total of 60k RGB images each of which are 32pixels x 32pixels x8bits per color channel. The small image size of 32 x 32 means that it is not very useful for real-world applications, but the CIFAR-10 dataset makes a good starting point for studying machine learning. The complete dataset of 60k images is normally divided into 50k images for training and 10k images for testing/validation.</p>
<p>There are a total of 10 mutually exclusive classes (or labels):</p>
<p>| Class index | Class name |
| :———: | ———- |
|      0      | airplane   |
|      1      | automobile |
|      2      | bird       |
|      3      | cat        |
|      4      | deer       |
|      5      | dog        |
|      6      | frog       |
|      7      | horse      |
|      8      | ship       |
|      9      | truck      |</p>
</div>
<div class="section" id="the-densenetx-cnn">
<h1>The DenseNetX CNN<a class="headerlink" href="#the-densenetx-cnn" title="Permalink to this headline">¶</a></h1>
<p>The DenseNet architecture was first proposed by Huang et al in their paper <a class="reference external" href="https://arxiv.org/pdf/1608.06993.pdf">‘Densely Connected Convolutional Networks’</a> The code provided in the <code class="docutils literal notranslate"><span class="pre">DenseNetX.py</span></code> python script is based on version 5 of the paper and includes the Bottleneck layers and Compression factor, so strictly speaking is a DenseNet-BC implementation. It also includes some minor modifications to make it compatible with the Vitis AI quantizer and compiler. In particular, the order of the BatchNorm, RelU activation and Convolution layers has been altered from BN-&gt;ReLU-&gt;Conv to Conv-&gt;BN-&gt;ReLU.</p>
<p>The authors of the original paper used a Stochastic Gradient Descent (SGD) optimizer whereas the training script in this example (<code class="docutils literal notranslate"><span class="pre">train.py</span></code>) uses RMSProp - the code required for SGD optimization is also provided, just uncomment it and then comment out the RMSProp optimizer, like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Optimizer</span>
<span class="sd">    RMSprop used in this example.</span>
<span class="sd">    SGD  with Nesterov momentum was used in original paper</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learnrate</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#opt = RMSprop(lr=learnrate)</span>
</pre></div>
</div>
<p>The code in <code class="docutils literal notranslate"><span class="pre">DenseNetX.py</span></code> is set for the CIFAR10 dataset, so the first convolutional layer uses a 3x3 kernel with a stride length of 1 and the first max pooling layer is ommitted:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># Use this for CIFAR-10, CIFAR-100</span>
    <span class="c1"># first convolutional layer + BN + ReLU (Imagenet style)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">k</span><span class="p">),</span><span class="mi">3</span><span class="p">,</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;he_uniform&#39;</span><span class="p">,</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p>In the original DenseNet paper, the first convolutional layer was set for a 7x7 kernel, stride lengths of 2 and a max pooling layer was included for the ImageNet dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># Use this for IMAGENET</span>
    <span class="c1"># first convolutional layer + BN + ReLU</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">k</span><span class="p">),</span> <span class="mi">7</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>

    <span class="c1"># max pooling layer</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note</strong>: You must adjust this code according to the size of your input data.</p>
<p>A picture of the complete network can be found at ./img/model.png.</p>
<div class="section" id="densenet-121-169-201-and-264-naming">
<h2>DenseNet-121, 169, 201 and 264 Naming<a class="headerlink" href="#densenet-121-169-201-and-264-naming" title="Permalink to this headline">¶</a></h2>
<p>The authors of the original paper labelled the architectures they created for ImageNet as DenseNet-121, 169, 201 and 264.  (Reference 1, Table 1). It can be useful to understand how they arrived at that nomenclature.</p>
<p>They used the parameter ‘L’ to indicate the depth of their networks but counted only the convolutional and fully-connected layers, they did not include the pooling, batch norm or activation layers.</p>
<p>For example, DenseNet-121 is made up of:</p>
<ul class="simple">
<li><p>the first convolutional layer = 1</p></li>
<li><p>1 dense block of 6 1x1 and 3x3 convolutional layers = 6 x 2 = 12.</p></li>
<li><p>1 transition layer with 1 convolutional layer = 1.</p></li>
<li><p>1 dense block of 12 1x1 and 3x3 convolutional layers = 12 x 2 = 24.</p></li>
<li><p>1 transition layer with 1 convolutional layer = 1.</p></li>
<li><p>1 dense block of 24 1x1 and 3x3 convolutional layers = 24 x 2 = 48.</p></li>
<li><p>1 transition layer with 1 convolutional layer = 1.</p></li>
<li><p>1 dense block of 16 1x1 and 3x3 convolutional layers = 16 x 2 = 32.</p></li>
<li><p>1 classification layer which contains 1 fully-connected layer = 1</p></li>
</ul>
<p>The sum of 1 + 12 + 1 + 24 + 1 + 48 + 1 + 32 + 1 = 121</p>
<p>The DenseNetX model provided in this tutorial has 3 dense blocks each of which have 16 sets of 1x1 and 3x3 convolutions. The 3 dense blocks are created by the <code class="docutils literal notranslate"><span class="pre">convlayers</span></code> argument which must be provided as a Python list:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">model</span> <span class="o">=</span> <span class="n">densenetx</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_height</span><span class="p">,</span><span class="n">input_width</span><span class="p">,</span><span class="n">input_chan</span><span class="p">),</span><span class="n">classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span><span class="n">convlayers</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="implementing-the-design">
<h1>Implementing the Design<a class="headerlink" href="#implementing-the-design" title="Permalink to this headline">¶</a></h1>
<p>This section will lead you through the steps necessary to run the design in hardware.</p>
<div class="section" id="preparing-the-host-machine-and-target-board">
<h2>Preparing the Host Machine and Target Board<a class="headerlink" href="#preparing-the-host-machine-and-target-board" title="Permalink to this headline">¶</a></h2>
<p>The host machine has several requirements that need to be met before we begin. You will need:</p>
<ul class="simple">
<li><p>An Ubuntu 16.04 or 18.04 x86 host machine with internet access to download files.</p></li>
<li><p>Optionally, a GPU card suitable for training (a trained checkpoint is provided for those who wish to skip the training step).</p></li>
<li><p>The environment is supposed to be ready at this step. If not, please follow the setup instructions provided in <a class="reference external" href="../Module_2">Module_2</a> and in <a class="reference external" href="../Module_3">Module_3</a>.</p></li>
</ul>
</div>
<div class="section" id="downloading-the-design-and-setting-up-the-workspace">
<h2>Downloading the Design and Setting up the Workspace<a class="headerlink" href="#downloading-the-design-and-setting-up-the-workspace" title="Permalink to this headline">¶</a></h2>
<p>This repository should be downloaded to the host machine as a zip file and then unzipped to a folder, or cloned using the <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span></code> command from a terminal.</p>
<p>Open a linux terminal, cd into the repository folder then into the ‘files’ folder. Start the Vitis AI docker - if you have a GPU in the host system, it is recommended that you use the GPU version of the docker container. If you intend running the model training, you will definitely need the GPU docker container. If you are going to skip the training phase, then the CPU docker container will be sufficient.</p>
<p>As part of the <a class="reference external" href="https://www.xilinx.com/html_docs/vitis_ai/1_2/jck1570690043273.html">Setting up the host</a> procedure, you will have cloned or downloaded The Vitis AI repository to the host machine. In the Vitis AI folder of that repo there is a shell script called docker_run.sh that will launch the chosen docker container. Open a terminal on the host machine and cd into the enter the following commands (note: start <em>either</em> the GPU or the CPU docker container, but not both):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># navigate to densenet tutorial folder</span>
<span class="nb">cd</span> &lt;path_to_densenet_design&gt;/DenseNetX

<span class="c1"># to start GPU docker</span>
&lt;path_to_Vitis-AI&gt;/docker_run.sh xilinx/vitis-ai-gpu:latest

<span class="c1"># ..or to start CPU docker</span>
&lt;path_to_Vitis-AI&gt;/docker_run.sh xilinx/vitis-ai-cpu:latest
</pre></div>
</div>
<p>The docker container will start and you should see something like this in the terminal:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">==========================================</span>

__      ___ _   _                   _____
<span class="se">\ \ </span>   / <span class="o">(</span>_<span class="o">)</span> <span class="p">|</span> <span class="o">(</span>_<span class="o">)</span>            /<span class="se">\ </span>  <span class="p">|</span>_   _<span class="p">|</span>
 <span class="se">\ \ </span> / / _<span class="p">|</span> <span class="p">|</span>_ _ ___ ______ /  <span class="se">\ </span>   <span class="p">|</span> <span class="p">|</span>
  <span class="se">\ \/</span> / <span class="p">|</span> <span class="p">|</span> __<span class="p">|</span> / __<span class="p">|</span>______/ /<span class="se">\ \ </span>  <span class="p">|</span> <span class="p">|</span>
   <span class="se">\ </span> /  <span class="p">|</span> <span class="p">|</span> <span class="p">|</span>_<span class="p">|</span> <span class="se">\_</span>_ <span class="se">\ </span>    / ____ <span class="se">\ </span>_<span class="p">|</span> <span class="p">|</span>_
    <span class="se">\/</span>   <span class="p">|</span>_<span class="p">|</span><span class="se">\_</span>_<span class="p">|</span>_<span class="p">|</span>___/    /_/    <span class="se">\_\_</span>____<span class="p">|</span>

<span class="o">==========================================</span>

Docker Image Version:  latest
Build Date: <span class="m">2020</span>-12-25
VAI_ROOT: /opt/vitis_ai

For TensorFlow Workflows <span class="k">do</span>:
     conda activate vitis-ai-tensorflow
For Caffe Workflows <span class="k">do</span>:
     conda activate vitis-ai-caffe
For Neptune Workflows <span class="k">do</span>:
     conda activate vitis-ai-neptune
For PyTorch Workflows <span class="k">do</span>:
     conda activate vitis-ai-pytorch
For TensorFlow <span class="m">2</span>.3 Workflows <span class="k">do</span>:
     conda activate vitis-ai-tensorflow2
For Darknet Optimizer Workflows <span class="k">do</span>:
     conda activate vitis-ai-optimizer_darknet
For Caffe Optimizer Workflows <span class="k">do</span>:
     conda activate vitis-ai-optimizer_caffe
For TensorFlow <span class="m">1</span>.15 Workflows <span class="k">do</span>:
     conda activate vitis-ai-optimizer_tensorflow
For LSTM Workflows <span class="k">do</span>:
     conda activate vitis-ai-lstm
Vitis-AI /workspace &gt;
</pre></div>
</div>
<p>Now run the environment setup script:  <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./0_setenv.sh</span></code></p>
<p>This will set up all the environment variables (..mainly pointers to folder and files..) most of which you can edit as required. It will also create the folders for the logs and the trained keras checkpoint.</p>
<p>The 0_setenv.sh script also activates the ‘vitis-ai-tensorflow’ TensorFlow conda environment, so you should now see that the terminal prompt looks like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>vitis-ai-tensorflow<span class="o">)</span> Vitis-AI /workspace$
</pre></div>
</div>
</div>
<div class="section" id="step-1-training-your-model">
<h2>Step 1: Training Your Model<a class="headerlink" href="#step-1-training-your-model" title="Permalink to this headline">¶</a></h2>
<p>:pushpin: Training takes a considerable time, between 8-12 hours depending on the GPU. You can either:</p>
<ul class="simple">
<li><p>Reduce the number of epochs by editing the <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">EPOCHS=160</span></code> line in the 0_setenv.sh shell script. Obviously, less epochs of training will have a negative impact on the final accuracy.</p></li>
<li><p>Skip the training phase altogether and use the pretrained Keras checkpoint available in keras_model.zip. In this case, you can copy the k_model.h5 file inside this zip archive to the ./files/build/keras_model folder. You can then skip the remaining parts of Step 1 and go directly to Step 2.</p></li>
</ul>
<p>To run step 1: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./1_train.sh</span></code></p>
<p>The training process is defined by the train.py Python script. First, the CIFAR10 dataset is downloaded and pre-processed by the datadownload.py script which is called by train.py. All images undergo simple pre-processing before being used for training, evaluation and quantization calibration. The images are normalized to bring all pixel values into the range 0 to 1.0 by dividing them by 255.</p>
<p>The images will also undergo augmentation in the form of random rotation, horizontal flipping (i.e. around the <em>vertical</em> axis), shifting up and down, shearing and zooming:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data_augment</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rotation_range</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                  <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                  <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                  <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                  <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                  <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">train_generator</span> <span class="o">=</span> <span class="n">data_augment</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
                                    <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batchsize</span><span class="p">,</span>
                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>At the end of training, the accuracy of the trained model will be calculated using the .evaluate method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Evaluation Loss    : &#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Evaluation Accuracy: &#39;</span><span class="p">,</span> <span class="n">scores</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>After training has finished, the trained Keras checkpoint will be found in the ./files/build/keras_model folder as an HDF5 file called k_model.h5.</p>
<p><em>Note: Any error messages relating to CUPTI can be ignored.</em></p>
</div>
<div class="section" id="step-2-converting-the-keras-hdf5-checkpoint-to-a-tensorflow-frozen-graph">
<h2>Step 2: Converting the Keras HDF5 Checkpoint to a TensorFlow Frozen Graph<a class="headerlink" href="#step-2-converting-the-keras-hdf5-checkpoint-to-a-tensorflow-frozen-graph" title="Permalink to this headline">¶</a></h2>
<p>To run step 2: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./2_keras2tf.sh</span></code></p>
<p>The Vitis AI tools cannot operate directly on Keras checkpoints and require a TensorFlow compatible frozen graph as the input format. The 2_keras2tf.sh shell script will create the frozen graph in two steps:</p>
<ol class="simple">
<li><p>The HDF5 file is converted to a TensorFlow checkpoint.</p></li>
<li><p>The TensorFlow checkpoint is converted to a ‘frozen graph’ in binary protobuf format.</p></li>
</ol>
<p>The output .pb file is generally known as a ‘frozen graph’ since all variables are converted into constants and graph nodes associated with training such as the optimizer and loss functions are stripped out.</p>
<p>After this step is completed, there should be a protobuf file called ‘frozen_graph.pb’ in the ./files/build/freeze folder.</p>
</div>
<div class="section" id="step-3-evaluating-the-frozen-graph">
<h2>Step 3: Evaluating the Frozen Graph<a class="headerlink" href="#step-3-evaluating-the-frozen-graph" title="Permalink to this headline">¶</a></h2>
<p>To run step 3: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./3_eval_frozen.sh</span></code></p>
<p>This is an optional step as the frozen graph is still in floating-point format and should give almost identical accuracy results as the evaluation done during the training phase (step 1). All 10k images of CIFAR10 test images are passed through the frozen model and the accuracy is calculated.</p>
</div>
<div class="section" id="step-4-quantizing-the-frozen-graph">
<h2>Step 4: Quantizing the Frozen Graph<a class="headerlink" href="#step-4-quantizing-the-frozen-graph" title="Permalink to this headline">¶</a></h2>
<p>To run step 4: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./4_quant.sh</span></code></p>
<p>The DPU accelerator IP executes all calculations in 8bit integer format, so we must quantize our floating-point frozen graph. This is done by the Vitis AI tools, in particular by the ‘vai_q_tensorflow quantize’ command. This command can be seen in the 4_quant.sh script and has several arguments that we must provide values for:</p>
<p>| Argument               | Description                                                    |
| ———————- | ————————————————————– |
| <code class="docutils literal notranslate"><span class="pre">--input_frozen_graph</span></code> | path and name of the input  .pb frozen graph                   |
| <code class="docutils literal notranslate"><span class="pre">--input_fn</span></code>           | Name of input function used in calibration pre-processing      |
| <code class="docutils literal notranslate"><span class="pre">--output_dir</span></code>         | Name of the output folder where the quantized models are saved |
| <code class="docutils literal notranslate"><span class="pre">--input_nodes</span></code>        | Name(s) of the input nodes                                     |
| <code class="docutils literal notranslate"><span class="pre">--output_nodes</span></code>       | Name(s) of the output nodes                                    |
| <code class="docutils literal notranslate"><span class="pre">--input_shapes</span></code>       | Shape(s) of the input nodes                                    |
| <code class="docutils literal notranslate"><span class="pre">--calib_iter</span></code>         | Number of calibration iterations                               |</p>
<p><em>Note: Any error messages relating to ./bin/ptxas can be ignored.</em></p>
<p>Most of the arguments are self-explanatory but special mention needs to be made of –input_fn and –calib_iter.</p>
<p>We require a sample set of data to calibrate the quantization process. This data will be passed through the model in one forward pass and so must be processed in exactly the same way as the data is pre-processed in training…the function pointed to be the –input_fn argument will need to contain all of the pre-processing steps.</p>
<p>The images for the calibration are created by the tf_gen_images.py python script and then stored in the ./files/build/quantize/images folder along with a text file which lists those images. This folder will be deleted after quantization is finished to reduce occupied disk space.</p>
<p>The image_input_fn.py Python script contains a single function called calib_input (..hence we set –input_fn to image_input_fn.calib_input in the 4_quant.sh shell script..) which opens the images with OpenCV, flips them to RGB from BGR as the model was trained on RGB images and then normalizes them to have all pixels in the range 0 to 1.0, exactly as was done in training and evaluation.</p>
<p>The number of images generated for use in calibration is set by the CALIB_IMAGES environment variable in the 0_setenv.sh script. Care should be taken that the number of calibration iterations (–calib_iter) multiplied by the calibration batch size (set in the image_input_fn.py script) does not exceed the total number of available images (CALIB_IMAGES).</p>
<p>Once quantization has completed, we will have the quantized deployment model (deploy_model.pb) and the evaluation model (quantize_eval_model.pb) in the ./files/build/quantize folder.</p>
</div>
<div class="section" id="step-5-evaluating-the-quantized-model">
<h2>Step 5: Evaluating the Quantized Model<a class="headerlink" href="#step-5-evaluating-the-quantized-model" title="Permalink to this headline">¶</a></h2>
<p>To run step 5: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./5_eval_quant.sh</span></code></p>
<p>This is an optional, but <em>highly</em> recommended step. The conversion from a floating-point model where the values can have a very wide dynamic range to an 8bit model where values can only have one of 256 values almost inevitably leads to a small loss of accuracy. We use the quantized evaluation model to see exactly how much impact the quantization has had.</p>
<p>The exact same Python script, eval_graph.py, that was used to evaluate the frozen graph is used to evaluate the quantized model.</p>
</div>
<div class="section" id="step-6-compiling-the-quantized-model">
<h2>Step 6: Compiling the Quantized Model<a class="headerlink" href="#step-6-compiling-the-quantized-model" title="Permalink to this headline">¶</a></h2>
<p>To run step 6: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./6_compile.sh</span></code></p>
<p>The DPU IP is a soft-core IP whose only function is to accelerate the execution of convolutional neural networks. It is a co-processor which has its own instruction set - those instructions are passed to the DPU in Xmodel file format.</p>
<p>The Vitis AI compiler will convert, and optimize where possible, the quantized deployment model to a set of micro-instructions and then output them in an Xmodel file.</p>
<p>The generated instructions are specific to the particular configuration of the DPU. The DPU’s parameters are contained in a arch.json file which needs to be created for each target board - see the <a class="reference external" href="https://www.xilinx.com/html_docs/vitis_ai/1_3/zmw1606771874842.html">Vitis AI User Guide</a> for details.</p>
<p>In the specific case of the ZCU104 and the prepared SDcard image file that we used back in the ‘Preparing the host machine and target board’ section, the arch.json file is included in the docker container and its location is passed to the vai_c_tensorflow command via the –arch argument.</p>
<p>Once compile is complete, the Xmodel file will be stored in the ./build/compile folder.</p>
</div>
<div class="section" id="step-7-running-the-application-on-the-board">
<h2>Step 7: Running the Application on the Board<a class="headerlink" href="#step-7-running-the-application-on-the-board" title="Permalink to this headline">¶</a></h2>
<p>To run step 7: <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">./7_make_target.sh</span></code></p>
<p>This final step will copy all the required files for running on the board into the ./build/target folder. The entire target folder will be copied to the ZCU104 SDcard. The 7_make_target.sh script also creates 10000 images from the CIFAR10 test set - the application code will preprocess and classify these images.</p>
<p>Copy it to the /home/root folder of the flashed SD card, this can be done with <code class="docutils literal notranslate"><span class="pre">scp</span></code> command.</p>
<ul class="simple">
<li><p>If the ZCU104 is connected to the same network as the host machine, the target folder can be copied using scp.</p></li>
<li><p>The command will be something like <code class="docutils literal notranslate"><span class="pre">scp</span> <span class="pre">-r</span> <span class="pre">./target</span> <span class="pre">root&#64;192.168.1.227:~/</span></code>  assuming that the ZCU104 IP address is 192.168.1.227 - adjust this and the path to the target folder as appropriate for your system.</p></li>
<li><p>If the password is asked for, insert ‘root’.</p></li>
</ul>
<p>With the target folder copied to the SD Card and the ZCU104 booted, you can issue the command for launching the application - note that this done on the ZCU104 board, not the host machine, so it requires a connection to the ZCU104 such as a serial connection to the UART or an SSH connection via Ethernet.</p>
<p>The application can be started by navigating into the target folder and then issuing the command <code class="docutils literal notranslate"><span class="pre">python3</span> <span class="pre">app_mt.py</span></code>. The application will start and after a few seconds will show the throughput in frames/sec.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@xilinx-zcu104-2020_2:~/target# python3 app_mt.py
Command line options:
 --image_dir :  images
 --threads   :  <span class="m">1</span>
 --model     :  model_dir/densenetx.xmodel
Pre-processing <span class="m">10000</span> images...
Starting <span class="m">1</span> threads...
<span class="nv">FPS</span><span class="o">=</span><span class="m">453</span>.83, total <span class="nv">frames</span> <span class="o">=</span> <span class="m">10000</span> , <span class="nv">time</span><span class="o">=</span><span class="m">22</span>.0348 seconds
output buffer length: <span class="m">10000</span>
Correct: <span class="m">9145</span> Wrong: <span class="m">855</span> Accuracy: <span class="m">0</span>.9145
</pre></div>
</div>
<p>For better throughput, the number of threads can be increased like this:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@xilinx-zcu104-2020_2:~/target# python3 app_mt.py -t <span class="m">5</span>
Command line options:
 --image_dir :  images
 --threads   :  <span class="m">5</span>
 --model     :  model_dir/densenetx.xmodel
Pre-processing <span class="m">10000</span> images...
Starting <span class="m">5</span> threads...
<span class="nv">FPS</span><span class="o">=</span><span class="m">792</span>.44, total <span class="nv">frames</span> <span class="o">=</span> <span class="m">10000</span> , <span class="nv">time</span><span class="o">=</span><span class="m">12</span>.6192 seconds
output buffer length: <span class="m">10000</span>
Correct: <span class="m">9145</span> Wrong: <span class="m">855</span> Accuracy: <span class="m">0</span>.9145
</pre></div>
</div>
</div>
</div>
<div class="section" id="accuracy-performance-results">
<h1>Accuracy &amp; Performance Results<a class="headerlink" href="#accuracy-performance-results" title="Permalink to this headline">¶</a></h1>
<p>The floating-point post-training and frozen graph evaluations can be compared to the INT8 post-quantization model and actual results obtained by the hardware model running on the ZCU104 board:</p>
<p>| Post-training (Float) | Frozen Graph (Float) | Quantized Model (INT8) | Hardware model (INT8) |
| :——————-: | :——————: | :——————–: | :——————-: |
|        92.94%         |        93.03%        |         92.66%         |       91.45  %        |</p>
<p>The approximate throughput (in frames/sec) for various batch sizes is shown below:</p>
<p>| Threads | Throughput (fps) |
| :—–: | :————–: |
|    1    |      478.47      |
|    2    |      708.57      |
|    3    |      790.29      |
|    4    |      820.57      |
|    5    |      829.56      |</p>
<p>Now, you could jump to <a class="reference external" href="../Module_5">Module_5</a> for a more practical use case.</p>
<p align="center"><sup>Copyright&copy; 2020 Xilinx</sup></p></div>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
															</div>
														</div>
														<div class="col-md-pull-6 col-lg-pull-6 col-md-6 col-lg-6">
															<span class="copyright">
                                  
                                  &copy; 2020–2021, Xilinx, Inc.
                              </span>
															<ul class="list-inline sub-menu">
																<li>
																	<a href="https://www.xilinx.com/about/privacy-policy.html">Privacy</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/legal.html">Legal</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/contact.html">Contact</a>
																</li>
															</ul>
														</div>
													</div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="quicklinks parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<noindex>
						<span class="quickLinks">
							<ul>
								<li>
									<a href="#top" class="btn backToTop">
									<span class="fas fa-angle-up" aria-hidden="true"></span>
									</a>
								</li>
							</ul>
						</span>
					</noindex>
				</div>
			</div>
		</div>
		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>