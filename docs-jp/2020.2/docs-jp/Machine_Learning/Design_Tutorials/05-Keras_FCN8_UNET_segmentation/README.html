


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		
		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

  
  
  
  

  
      <script type="text/javascript" src="../../../_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="../../../_static/js/gtm.js"></script>
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="../../../_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/header-footer.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/linkid.js"></script>
    <script type="text/javascript" src="../../../_static/js/Searchbox.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pro.min.css" media="all" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="header parbase aem-GridColumn aem-GridColumn--default--12">
								<noindex>
									<header data-component="header">
										<nav class="navbar navbar-default aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid main-nav">
													<div class="row">
														<div class="col-xs-12">
															<div class="logo-column">
																<div class="logo">
																	<a href="https://www.xilinx.com/">
																	<img src="https://www.xilinx.com/etc.clientlibs/site/clientlibs/xilinx/all/resources/imgs/header/xilinx-header-logo.svg" title="Xilinx Inc"/>
																	</a>
																</div>
															</div>
															<div class="navbar-column">
																<div class="navbar navbar-collapse collapse" id="xilinx-main-menu">
																	<div class="mobile-search-container">
																		<div id="headerSearchBox" class="headerSearch"
																			data-component="header-search"
																			data-redirect-if-empty="false"
																			data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																			data-coveo-organization-id="xilinxcomprode2rjoqok">
																			<div class='coveo-search-section'>
																				<div class="CoveoAnalytics" data-search-hub="Site"></div>
																				<ul class="dropdown-menu options">
																					<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																						<a href="#">
																						All</a>
																					</li>
																					<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com//products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Silicon Devices</a>
																					</li>
																					<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com//products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Boards and Kits</a>
																					</li>
																					<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com//products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Intellectual Property</a>
																					</li>
																					<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																						<a href="#">
																						Support</a>
																						<ul>
																							<li data-label="Documentation" data-action-link="https://www.xilinx.com//support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																								<a href="#">
																								Documentation</a>
																							</li>
																							<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com//support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																								<a href="#">
																								Knowledge Base</a>
																							</li>
																							<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																								<a href="#">
																								Community Forums</a>
																							</li>
																						</ul>
																					</li>
																					<li data-label="Partners" data-action-link="https://www.xilinx.com//alliance/member-keyword-search.html" data-search-hub="Partner">
																						<a href="#">
																						Partners</a>
																					</li>
																					<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																						<a href="#">
																						Videos</a>
																					</li>
																					<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																						<a href="#">
																						Press</a>
																					</li>
																				</ul>
																				<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																				<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																			</div>
																		</div>
																	</div>
																	<ul class="nav navbar-nav nav-justified">
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/applications.html">
																			Applications</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/products/silicon-devices.html">
																			Products</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://developer.xilinx.com/">
																			Developers</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/support.html">
																			Support</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/about/company-overview.html">
																			About</a>
																		</li>
																	</ul>
																</div>
															</div>
															<script type="text/javascript" src="../../../_static/js/gtm.js"></script>
															<!--<div class="mini-nav">
																<button type="button" data-function="xilinx-mobile-menu" id="nav-toggle" class="navbar-toggle collapsed visible-xs-block" aria-expanded="false">
																<span></span>
																<span></span>
																<span></span>
																<span></span>
																</button>
																<ul class="list-inline">
																	<li class="dropdown user-menu">
																		<button data-toggle="dropdown">
																		<span class="sr-only">Account</span>
																		<span class="fas fa-user"></span>
																		</button>
																		<ul class="dropdown-menu">
																			<li>
																				<a href="https://www.xilinx.com/myprofile/subscriptions.html">
																				My Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/registration/create-account.html">
																				Create Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/bin/protected/en/signout">
																				Sign Out</a>
																			</li>
																		</ul>
																	</li>
																	<li class="hidden-xs">
																		<button data-function="search-toggle">
																		<span class="sr-only">Search</span>
																		<span class="far fa-search"></span>
																		</button>
																	</li>
																</ul>
															</div>
															-->
															<div class="search-container">
																<div id="headerSearchBox" class="headerSearch"
																	data-component="header-search"
																	data-redirect-if-empty="false"
																	data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																	data-coveo-organization-id="xilinxcomprode2rjoqok">
																	<div class='coveo-search-section'>
																		<div class="CoveoAnalytics" data-search-hub="Site"></div>
																		<ul class="dropdown-menu options">
																			<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																				<a href="#">
																				All</a>
																			</li>
																			<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com/products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Silicon Devices</a>
																			</li>
																			<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com/products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Boards and Kits</a>
																			</li>
																			<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com/products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Intellectual Property</a>
																			</li>
																			<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																				<a href="#">
																				Support</a>
																				<ul>
																					<li data-label="Documentation" data-action-link="https://www.xilinx.com/support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																						<a href="#">
																						Documentation</a>
																					</li>
																					<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com/support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																						<a href="#">
																						Knowledge Base</a>
																					</li>
																					<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																						<a href="#">
																						Community Forums</a>
																					</li>
																				</ul>
																			</li>
																			<li data-label="Partners" data-action-link="https://www.xilinx.com/alliance/member-keyword-search.html" data-search-hub="Partner">
																				<a href="#">
																				Partners</a>
																			</li>
																			<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																				<a href="#">
																				Videos</a>
																			</li>
																			<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																				<a href="#">
																				Press</a>
																			</li>
																		</ul>
																		<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																		<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																	</div>
																</div>
																<button data-function="search-toggle">
																<span class="sr-only">Search</span>
																<span class="far fa-times"></span>
																</button>
															</div>
														</div>
													</div>
												</div>
											</div>
										</nav>
									</header>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Vitis チュートリアル
          

          
          </a>

          
            
            
              <div class="version">
                2020.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <p class="caption"><span class="caption-text">English version</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs/index.html">Master</a></li>
</ul>
<p class="caption"><span class="caption-text">入門</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis/README.html">Vitis フロー 101 チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis_HLS/README.html">Vitis HLS の解析および最適化</a></li>
</ul>
<p class="caption"><span class="caption-text">機械学習 (英語版)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Introduction to Machine Learning with Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">アクセラレーション</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html">Vitis ハードウェア アクセラレータの概要</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#id1">設計チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#id2">機能チュートリアル</a></li>
</ul>
<p class="caption"><span class="caption-text">AI エンジン開発 (英語版)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/README.html">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">プラットフォーム作成チュートリアル</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Vitis_Platform_Creation/README.html">プラットフォームの作成</a></li>
</ul>
<p class="caption"><span class="caption-text">XRT および Vitis システム最適化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Runtime_and_System_Optimization/README.html">設計チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Runtime_and_System_Optimization/README.html#id2">機能チュートリアル</a></li>
</ul>
<p class="caption"><span class="caption-text">バージョン</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-1/docs/README.html">2020.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-Tutorials/blob/Vitis-Tutorials-2019.2-Hotfix1/README.md">2019.2</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="../../../_sources/Machine_Learning/Design_Tutorials/05-Keras_FCN8_UNET_segmentation/README.md.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Vitis チュートリアル</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Current status</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Machine_Learning/Design_Tutorials/05-Keras_FCN8_UNET_segmentation/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div style="page-break-after: always;"></div>
<table style="width:100%">
  <tr>
    <th width="100%" colspan="6"><img src="https://www.xilinx.com/content/dam/xilinx/imgs/press/media-kits/corporate/xilinx-logo.png" width="30%"/><h1>FCN8 and UNET Semantic Segmentation with Keras and Xilinx Vitis AI</h1>
</th>
  </tr>
</table>
</div><div class="section" id="current-status">
<h1>Current status<a class="headerlink" href="#current-status" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p>Tested with Vitis AI 1.3.</p></li>
<li><p>Fully working in hardware on ZCU102. It should work also on ZCU104.</p></li>
<li><p>In hardware on VCK190 only FCN8 works fine.</p></li>
</ol>
<div class="section" id="date-8-jan-2021">
<h2>Date: 8 Jan 2021<a class="headerlink" href="#date-8-jan-2021" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="introduction">
<h1>1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>In this Deep Learning (DL) tutorial, you will train the <a class="reference external" href="http://deeplearning.net/tutorial/fcn_2D_segm.html">FCN8</a> and <a class="reference external" href="https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47">UNET</a> Convolutional Neural Networks (CNNs) for Semantic Segmentation in Keras adopting a small custom dataset, then you will quantize the floating point weights files to an 8-bit fixed point  representation to finally deploy them on the Xilinx® target board using Vitis™ AI, which is a set of optimized IP, tools libraries, models and example designs valid for AI inference on both Xilinx edge devices and Alveo cards. See the <a class="reference external" href="https://developer.xilinx.com/en/get-started/ai.html">Vitis AI Product Page</a> for more information.</p>
<p>Once the selected CNN has been correctly trained in Keras, the <a class="reference external" href="https://www.hdfgroup.org/solutions/hdf5/">HDF5</a> file of weights is converted into a TF checkpoint and inference graph file, such frozen graph is then quantized by the Vitis AI Quantizer that creates an INT8 <code class="docutils literal notranslate"><span class="pre">pb</span></code> file from which the Vitis AI Compiler generates the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file of micro instructions for the Deep Processor Unit (DPU) of the Vitis AI platform. The final application is executed at run time - via Python APIs - on the target board to make predictions that can be compared against the simulation reference results.</p>
</div>
<div class="section" id="prerequisites">
<h1>2 Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Ubuntu 16.04 host PC with Python 3.6.</p></li>
<li><p>The entire repository of <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.3</a> from <a class="reference external" href="https://www.github.com/Xilinx">www.github.com/Xilinx</a>.</p></li>
<li><p>Accurate reading of <a class="reference external" href="https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf">Vitis AI User Guide UG1414 v1.3</a>. In particular:</p></li>
</ul>
<ol class="simple">
<li><p>“Vitis AI Overview” in Chapter 1 with DPU naming and guidelines to download the tools container available from <a class="reference external" href="https://hub.docker.com/r/xilinx/vitis-ai/tags">docker hub</a> and the Runtime Package for edge (MPSoC) devices.</p></li>
<li><p>“Installation and Setup” instructions of Chapter 2 for both host and target;</p></li>
<li><p>“Quantizing the Model” in Chapter 4 and “Compiling the Model” in Chapter 5.</p></li>
<li><p>“Programming with VART” APIs in Chapter 6.</p></li>
</ol>
<ul class="simple">
<li><p>A Vitis AI Evaluation board such as the <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">ZCU102</a> with its <a class="reference external" href="https://www.xilinx.com/bin/public/openDownload?filename=xilinx-zcu102-dpu-v2020.2-r1.3.0.2.0.img.gz">image file</a>, which contains a pre-built working design for the ZCU102 with the DPUCZDX8G (renamed shortly as “DPUv2” in the following).</p></li>
<li><p>Familiarity with Deep Learning principles.</p></li>
</ul>
<div class="section" id="dos-to-unix-conversion">
<h2>Dos-to-Unix Conversion<a class="headerlink" href="#dos-to-unix-conversion" title="Permalink to this headline">¶</a></h2>
<p>In case you might get some strange errors during the execution of the scripts, you have to pre-process -just once- all the<code class="docutils literal notranslate"><span class="pre">*.sh</span></code> shell and the python <code class="docutils literal notranslate"><span class="pre">*.py</span></code> scripts with the <a class="reference external" href="http://archive.ubuntu.com/ubuntu/pool/universe/d/dos2unix/dos2unix_6.0.4.orig.tar.gz">dos2unix</a> utility.
In that case run the following commands from your Ubuntu host PC (out of the Vitis AI docker images):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo apt-get install dos2unix
<span class="nb">cd</span> &lt;WRK_DIR&gt; <span class="c1">#your working directory</span>
<span class="k">for</span> file in <span class="k">$(</span>find . -name <span class="s2">&quot;*.sh&quot;</span><span class="k">)</span><span class="p">;</span> <span class="k">do</span>
  dos2unix <span class="si">${</span><span class="nv">file</span><span class="si">}</span>
<span class="k">done</span>
</pre></div>
</div>
</div>
<div class="section" id="vitis-ai-1-2">
<h2>Vitis AI 1.2<a class="headerlink" href="#vitis-ai-1-2" title="Permalink to this headline">¶</a></h2>
<p>If you need to use the older Vitis AI 1.2 release, just replace this <code class="docutils literal notranslate"><span class="pre">README.md</span></code> file with the one placed in the subfolder
<code class="docutils literal notranslate"><span class="pre">vai_1v2</span></code> and go on in following the instructions on that file and the related <code class="docutils literal notranslate"><span class="pre">vai_1v2.zip</span></code> archive, then skip the rest of this document.</p>
</div>
</div>
<div class="section" id="before-starting-with-vitis-ai-1-3">
<h1>3 Before starting with Vitis AI 1.3<a class="headerlink" href="#before-starting-with-vitis-ai-1-3" title="Permalink to this headline">¶</a></h1>
<p>In the following of this document, it is assumed that you have cloned the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.3</a> in your working directory <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code>, for example a folder named <code class="docutils literal notranslate"><span class="pre">~/ML/VAI1v3</span></code>.</p>
<p>It is also assumed that your target board is called just “TARGET”, once not specified in more details.</p>
<p>To list the currently available docker images run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker images <span class="c1"># to list the current docker images available in the host pc</span>
</pre></div>
</div>
<p>and you should see something like in the following text:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>REPOSITORY                      TAG                               IMAGE ID            CREATED             SIZE
xilinx/vitis-ai-gpu             1.3                               f42fb3515bcd        About an hour ago   27.5GB
</pre></div>
</div>
<p>To launch the docker container with Vitis AI tools, based on CPU (or GPU), execute the following commands from the <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt; <span class="c1"># you are now in Vitis_AI subfolder</span>
./docker_run.sh xilinx/vitis-ai-gpu:1.3
</pre></div>
</div>
<p>Note that the container maps the shared folder <code class="docutils literal notranslate"><span class="pre">/workspace</span></code> with the file system of the Host PC from where you launch the above command, which is <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> in your case.
This shared folder enables you to transfer files from the Host PC to the docker container and vice versa.</p>
<p>The docker container do not have any graphic editor, so it is recommended that you work with two terminals and you point to the same folder, in one terminal you use the docker container commands and in the other terminal you open any graphic editor you like.</p>
<p>Note that docker does not have an automatic garbage collection system as of now. You can use this command to do a manual garbage collection:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker rmi -f $(docker images -f &quot;dangling=true&quot; -q)
</pre></div>
</div>
<p>Starting from Vitis AI 1.1 release there is no more Docker Runtime Container, and you can cross compile the application files directly from the Xilinx <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> environment on your host PC to the target board.
In the following of this tutorial it is assumed that <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> is installed into <code class="docutils literal notranslate"><span class="pre">~/petalinux_sdk</span></code> of your host PC, as recommended in <a class="reference external" href="https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf">UG1414</a>.</p>
<div class="section" id="install-missing-packages-on-the-vitis-ai-tools-container">
<h2>3.1 Install Missing Packages on the Vitis AI Tools Container<a class="headerlink" href="#install-missing-packages-on-the-vitis-ai-tools-container" title="Permalink to this headline">¶</a></h2>
<p>This tutorial requires some packages that were not included in the original Vitis AI tools container. Here are the commands to include such packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./docker_run.sh xilinx/vitis-ai-gpu:1.3     
sudo su <span class="c1"># you must be root</span>
conda activate vitis-ai-tensorflow <span class="c1"># as root, enter into Vitis AI TF (anaconda-based) virtual environment</span>
conda install seaborn
conda deactivate
<span class="nb">exit</span> <span class="c1"># to exit from root</span>
conda activate vitis-ai-tensorflow <span class="c1"># as normal user, enter into Vitis AI TF (anaconda-based) virtual environment</span>
</pre></div>
</div>
<p>Note that if you exit from the current Docker Vitis AI tools image you will lose all the installed packages, so to save all changes in a new docker image open a new terminal and run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo docker ps -l <span class="c1"># To get the Docker CONTAINER ID</span>
</pre></div>
</div>
<p>you will see the following text (the container ID might have a different number):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONTAINER ID        IMAGE                     COMMAND             CREATED             STATUS              PORTS               NAMES
967212b35a06        xilinx/vitis-ai-gpu:1.3   &quot;bash&quot;              5 minutes ago       Up 5 minutes                            admiring_minsky
</pre></div>
</div>
<p>now save the modified docker image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo docker commit -m<span class="s2">&quot;comment&quot;</span> 7c9927375b06 xilinx/vitis-ai-gpu:1.3
</pre></div>
</div>
<p>Assuming you have renamed this project <code class="docutils literal notranslate"><span class="pre">VAI-KERAS-FCN8-SEMSEG</span></code> and placed it in the directory named <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;/tutorials/</span></code> so that it is two levels below the <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> folder, you can launch the modified tools container by running the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt;
./docker_run.sh xilinx/vitis-ai-gpu:latest
<span class="nb">cd</span> /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG
conda activate vitis-ai-tensorflow
</pre></div>
</div>
</div>
</div>
<div class="section" id="the-main-flow">
<h1>4 The Main Flow<a class="headerlink" href="#the-main-flow" title="Permalink to this headline">¶</a></h1>
<p>The main flow is composed of seven major steps. The first six steps are executed from the tools container on the host PC by launching one of the scripts <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> or <a class="reference external" href="files/run_fcn8ups.sh">run_fcn8ups.sh</a> or <a class="reference external" href="files/run_unet.sh">run_unet.sh</a> -respectively for FCN8, FCN8UPS (a custom, modified version of FCN8) and UNET CNNs- with commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> run_fcn8.sh     <span class="c1"># FCN8 CNN</span>
<span class="nb">source</span> run_fcn8ups.sh  <span class="c1"># FCN8UPS CNN</span>
<span class="nb">source</span> run_unet.sh     <span class="c1"># UNET CNN</span>
</pre></div>
</div>
<p>The seventh step can be executed either directly on the target board or in your <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> cross-compilation host environment.</p>
<p>Here is an overview of each step.</p>
<ol class="simple">
<li><p>For each dataset, organize the data into proper folders, such as <code class="docutils literal notranslate"><span class="pre">train</span></code> (for training), <code class="docutils literal notranslate"><span class="pre">val</span></code> (for validation during the training phase), <code class="docutils literal notranslate"><span class="pre">test</span></code> (for testing during the inference/prediction phase) and <code class="docutils literal notranslate"><span class="pre">calib</span></code> (for calibration during the quantization phase). See <a class="reference external" href="#41-organize-the-data">Organize the Data</a> for more information.</p></li>
<li><p>Train the CNNs in Keras and generate the HDF5 weights model. See <a class="reference external" href="#42-train-the-cnn">Train the CNN</a> for more information.</p></li>
<li><p>Convert the Keras model into TF checkpoint and inference graph. See <a class="reference external" href="#43-create-tf-inference-graphs-from-keras-models">Create TF Inference Graphs from Keras Models</a> for more information.</p></li>
<li><p>Freeze the TF graph to evaluate the CNN prediction accuracy as the reference starting point. See <a class="reference external" href="#44-freeze-the-tf-graphs">Freeze the TF Graphs</a> for more information.</p></li>
<li><p>Quantize from 32-bit floating point to 8-bit fixed point and evaluate the prediction accuracy of the quantized CNN. See <a class="reference external" href="#45-quantize-the-frozen-graphs">Quantize the Frozen Graphs</a> for more information.</p></li>
<li><p>Run the compiler to generate the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file for the target board from the quantized <code class="docutils literal notranslate"><span class="pre">pb</span></code> file. See <a class="reference external" href="#46-compile-the-quantized-models">Compile the Quantized Models</a> for more information.</p></li>
<li><p>Use either VART C++ or Python APIs to write the hybrid application for the ARM CPU, then cross-compile it in the <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> host environment.  The application is called “hybrid” because the ARM CPU is executing some software routines while the DPU hardware accelerator is running the FC, CONV, ReLU, and BN layers of the CNN that were coded in the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code>file.</p></li>
<li><p>Assuming you have archived the <code class="docutils literal notranslate"><span class="pre">target_zcu102</span></code> folder and transferred the related <code class="docutils literal notranslate"><span class="pre">target_zcu102.tar</span></code> archive from the host to the target board with <code class="docutils literal notranslate"><span class="pre">scp</span></code> utility, now you can run the hybrid application.
See <a class="reference external" href="#47-build-and-run-on-the-zcu102-target-board">Build and Run on the ZCU102 Target Board</a> for more information.</p></li>
</ol>
<blockquote>
<div><p><strong>:pushpin: NOTE</strong> Steps 1 and 2 are based on Yumi’s blog titled <a class="reference external" href="https://fairyonice.github.io/Learn-about-Fully-Convolutional-Networks-for-semantic-segmentation.html">Learn about Fully Convolutional Networks for semantic segmentation</a>. For more background information about Semantic Segmentation have a look at the <a class="reference external" href="#appendix">Appendix</a>.</p>
</div></blockquote>
<div class="section" id="organize-the-data">
<h2>4.1 Organize the Data<a class="headerlink" href="#organize-the-data" title="Permalink to this headline">¶</a></h2>
<p>You have to download the data from <a class="reference external" href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view">here</a> and save the file <code class="docutils literal notranslate"><span class="pre">dataset1.zip</span></code> (of size ~120MB) in the folder <a class="reference external" href="files">files</a> at the same level of other sub-folders like <a class="reference external" href="files/code">code</a> and <a class="reference external" href="files/log">log</a>.</p>
<p>The subroutine <code class="docutils literal notranslate"><span class="pre">1_generate_images()</span></code> within the script <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> calls the <a class="reference external" href="files/code/prepare_data.py">prepare_data.py</a> python module and creates the sub-folders: <code class="docutils literal notranslate"><span class="pre">img_train</span></code>, <code class="docutils literal notranslate"><span class="pre">img_valid</span></code>, <code class="docutils literal notranslate"><span class="pre">img_test</span></code>, and <code class="docutils literal notranslate"><span class="pre">img_calib</span></code> that are located in the <code class="docutils literal notranslate"><span class="pre">dataset1</span></code> directory and fills them with 311 images for training, 56 images for validation (taken from the images of the original training dataset), 101 images for testing (all the images of the original testing dataset), and 104 images for the calibration process (copied from the training images).</p>
<p>All the images are resized to 224x224x3 before being stored into their folders.</p>
<p>This tutorial applies only 12 classes in the dataset: “Sky”, “Wall”, “Pole”, “Road”, “Sidewalk”, “Vegetation”, “Sign”, “Fence”, “vehicle”, “Pedestrian”, “Bicyclist”, “miscellanea”; these classes are coded with the colors reported in Figure 2, which was generated with commented code from the <a class="reference external" href="files/code/config/fcn_config.py">fcn_config.py</a> script.</p>
<p>The following two lines of code from <a class="reference external" href="files/code/prepare_data.py">prepare_data.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cnn</span><span class="o">.</span><span class="n">plot_image_with_classes</span><span class="p">(</span><span class="n">dir_train_seg_inp</span><span class="p">,</span> <span class="n">dir_train_img_inp</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">plot_some_images</span><span class="p">(</span><span class="n">dir_train_seg_inp</span><span class="p">,</span> <span class="n">dir_train_img_inp</span><span class="p">)</span>
</pre></div>
</div>
<p>allow you to plot an image and its segmentation labels for each of the 12 classes (first line), and also to plot some examples of segmented images with their classes coded in colors (second line).</p>
<p><img alt="figure2a" src="../../../_images/segmentation_classes.png" /></p>
<p><img alt="figure2b" src="../../../_images/legend_rgb.png" /></p>
<p><em>Figure 2: Examples of segmentation labels on the top. Colors (right) to encode the Segmentation Classes (left) on the bottom.</em></p>
</div>
<div class="section" id="train-the-cnn">
<h2>4.2 Train the CNN<a class="headerlink" href="#train-the-cnn" title="Permalink to this headline">¶</a></h2>
<p>The input data are pre-processed using the following python code to normalize their values from 0 to 1. Such code has to be mirrored in the C++ or Python application that runs in the ARM® CPU of ZCU102 target board.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span>  <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<div class="section" id="fcn8-two-models">
<h3>4.2.1 FCN8 (two models)<a class="headerlink" href="#fcn8-two-models" title="Permalink to this headline">¶</a></h3>
<p>This tutorial proposes two variances of FCN8:</p>
<ol class="simple">
<li><p>the original model of Yumi’s blog -named FCN8- with the scheme illustrated in Figure 3 and with ~28 Millions of parameters,</p></li>
<li><p>my modified model -named FCN8UPS- with the scheme of Figure 4 and with ~15 Millions of parameters,
where in the second model the first two <code class="docutils literal notranslate"><span class="pre">Conv2DTrans</span></code> layers are replaced by <code class="docutils literal notranslate"><span class="pre">UpSampling2D</span></code> layers.</p></li>
</ol>
<p>Both models include a VGG16 CNN backbone, you have to download the HDF5 weights file from <a class="reference external" href="https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5">fchollet’s GitHub</a> and put it in the subfolder <a class="reference external" href="files/keras_model">keras_model</a>.</p>
<p>From the <code class="docutils literal notranslate"><span class="pre">2_fcn8_train()</span></code> subroutine within the  <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> script you can call the <a class="reference external" href="files/code/fcn8_training.py">fcn8_training.py</a> module with the flag either <code class="docutils literal notranslate"><span class="pre">upscale=&quot;False&quot;</span></code> to get the original FCN8 model or <code class="docutils literal notranslate"><span class="pre">upscale=&quot;True&quot;</span></code> to get the second model. All the related files and folders names will contain the substrings either <code class="docutils literal notranslate"><span class="pre">fcn8</span></code> or <code class="docutils literal notranslate"><span class="pre">fcn8ups</span></code> respectively. Similarly, whatever FCN8 model you need, just use one of the two commands below to run  the six steps of the deployment process from the host PC:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> run_fcn8.sh     <span class="c1"># original FCN8</span>
<span class="c1">#source run_fcn8ups.sh  # FCN8 with UpSampling2D</span>
</pre></div>
</div>
<p>Once the training is completed, the <a class="reference external" href="files/code/fcn8_make_predictions.py">fcn8_make_predictions.py</a> module makes predictions on both the test and validation datasets and you should get a  <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> accuracy respectively of 0.406 and 0.426, as reported in the logfile placed in <a class="reference external" href="files/log">log</a> folder for the original FCN8 CNN, and  0.406 and 0.427 for the FCN8 with <code class="docutils literal notranslate"><span class="pre">Upsampling2D</span></code> (named FCN8UPS).</p>
<p>The learning curves are illustrated in Figure 5.</p>
<p>Note that, being the training dataset pretty small (only 311 images), the prediction accuracy is not very good (in that case it should be at least <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span> <span class="pre">&gt;=</span> <span class="pre">0.5</span></code>), as reported in the following text fragment of the logfile, in fact some classes are not even predicted (i.e. the classes 02, 06, 09 and 10). All in all the purpose of this tutorial is to show you what steps are needed to go from the <code class="docutils literal notranslate"><span class="pre">.pb</span></code> weight file of a trained FCN8 CNN to the run time execution on the FPGA device of the target board.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>class ( 0)          Sky: #TP= 431733, #FP=  23570, #FN=  24240, IoU=0.900
class ( 1)         Wall: #TP=1097453, #FP= 108340, #FN= 206493, IoU=0.777
class ( 2)         Pole: #TP=      3, #FP=     57, #FN=  36417, IoU=0.000
class ( 3)         Road: #TP=1406806, #FP=  59897, #FN=  68213, IoU=0.917
class ( 4)     Sidewalk: #TP= 399167, #FP= 104818, #FN=  49266, IoU=0.721
class ( 5)   Vegetation: #TP= 786947, #FP= 108697, #FN=  39598, IoU=0.841
class ( 6)         Sign: #TP=   3784, #FP=   2894, #FN=  49608, IoU=0.067
class ( 7)        Fence: #TP=  54427, #FP=  40537, #FN= 101976, IoU=0.276
class ( 8)      vehicle: #TP=  73622, #FP= 142380, #FN=  20438, IoU=0.311
class ( 9)   Pedestrian: #TP=     44, #FP=    300, #FN=  36821, IoU=0.001
class (10)    Bicyclist: #TP=   2057, #FP=    838, #FN= 108915, IoU=0.018
class (11)  miscellanea: #TP=  39417, #FP= 179988, #FN=  30331, IoU=0.158
_________________
Mean IoU: 0.416
</pre></div>
</div>
<p><img alt="figure3" src="../../../_images/fcn8_model224x224.png" /></p>
<p><em>Figure 3: Block diagram of the original FCN8 CNN.</em></p>
<p><img alt="figure4" src="../../../_images/fcn8ups_model224x224.png" /></p>
<p><em>Figure 4: Block diagram of FCN8UPS CNN (with UpSampling2D replacing first two Conv2DTrans layers).</em></p>
<p><img alt="figure5a" src="../../../_images/fcn8ups_training_curves_224x224.png" />
<img alt="figure5b" src="../../../_images/fcn8_training_curves_224x224.png" /></p>
<p><em>Figure 5: Training curves for FCN8UPS (top) and FCN8 (bottom) CNNs.</em></p>
</div>
<div class="section" id="unet-three-models">
<h3>4.2.2 UNET (three models)<a class="headerlink" href="#unet-three-models" title="Permalink to this headline">¶</a></h3>
<p>There are three variants of UNET, as proposed in the <a class="reference external" href="files/code/config/unet.py">code/config/unet.py</a> file. The complete process is managed by the script  <a class="reference external" href="files/run_unet.sh">run_unet.sh</a>, similarly to what done for the two FCN8 CNNs.
Learning curves and block diagram of UNET-v2 model are illustrated in Figures 6 and 7.</p>
<p><img alt="figure6" src="../../../_images/unet_model2_224x224.png" /></p>
<p><em>Figure 6: Block diagram of a UNET-v2 CNN.</em></p>
<p><img alt="figure7" src="../../../_images/unet_model2_training_curves_224x224.png" /></p>
<p><em>Figure 7: Training curves for a UNET-v2 CNN.</em></p>
</div>
</div>
<div class="section" id="create-tf-inference-graphs-from-keras-models">
<h2>4.3 Create TF Inference Graphs from Keras Models<a class="headerlink" href="#create-tf-inference-graphs-from-keras-models" title="Permalink to this headline">¶</a></h2>
<p>The subroutine <code class="docutils literal notranslate"><span class="pre">3_fcn8_Keras2TF()</span></code> within the  <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> script gets the computation graph of the TF backend representing the Keras model and generates the output files named <code class="docutils literal notranslate"><span class="pre">infer_graph.pb</span></code> and <code class="docutils literal notranslate"><span class="pre">float_model.chkpt.*</span></code> which are then placed in the folder <code class="docutils literal notranslate"><span class="pre">./workspace/tf_chkpts/fcn8</span></code>. The generated logfile in the <a class="reference external" href="files/log">log</a> folder also contains the TF names of the  input and output nodes that are needed to <a class="reference external" href="#freeze-the-tf-graphs">Freeze the TF Graphs</a>. For example, in the case of FCN8, such nodes are called <code class="docutils literal notranslate"><span class="pre">input_1</span></code> and <code class="docutils literal notranslate"><span class="pre">activation_1/truediv</span> </code> respectively.</p>
</div>
<div class="section" id="freeze-the-tf-graphs">
<h2>4.4 Freeze the TF Graphs<a class="headerlink" href="#freeze-the-tf-graphs" title="Permalink to this headline">¶</a></h2>
<p>The inference graph created in <a class="reference external" href="#create-tf-inference-graphs-from-keras-models">Create TF Inference Graphs from Keras Models</a> is first converted to a <a class="reference external" href="https://www.tensorflow.org/guide/extend/model_files">GraphDef protocol buffer</a>, then cleaned so that the subgraphs that are not necessary to compute the requested outputs, such as the training operations, can be removed. This process is called “freezing the graph”.</p>
<p>The subroutines <code class="docutils literal notranslate"><span class="pre">4a_fcn8_freeze()</span></code> and <code class="docutils literal notranslate"><span class="pre">4b_fcn8_eval_graph()</span></code> of <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> script generate the frozen graph and use it to evaluate the accuracy of the CNN by making predictions on the images in the <code class="docutils literal notranslate"><span class="pre">img_test</span></code> folder.</p>
<p>It is important to apply the correct <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">node</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">node</span></code> names in all the shell scripts. This information can be captured by this  python code fragment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the input and output name</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> TF input node name:&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> TF output node name:&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>The frozen graphs evaluation with <a class="reference external" href="files/code/eval_graph.py">eval_graph.py</a> generates a <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> prediction accuracy of 0.406 and 0.406 for the first and second variant of FCN8 CNN, as reported in the  logfile.</p>
</div>
<div class="section" id="quantize-the-frozen-graphs">
<h2>4.5 Quantize the Frozen Graphs<a class="headerlink" href="#quantize-the-frozen-graphs" title="Permalink to this headline">¶</a></h2>
<p>The subroutines <code class="docutils literal notranslate"><span class="pre">5a_fcn8_quantize()</span></code> and <code class="docutils literal notranslate"><span class="pre">5b_fcn8_eval_quantized_graph()</span></code> within the  <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> script generate the quantized graph and use it to evaluate the accuracy of the CNN by making predictions on the images from the <code class="docutils literal notranslate"><span class="pre">img_test</span></code> folder.</p>
<p>The quantized graphs evaluation with <a class="reference external" href="files/code/eval_quantized_graph.py">eval_quantized_graph.py</a> produces a <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> prediction accuracy again of 0.407 and 0.404, for the first and second variant of FCN8 CNN, as reported respectively in the logfile.</p>
<p>The prediction accuracy of the quantized network can be evaluated by few changes to the original python module <a class="reference external" href="files/code/eval_graph.py">eval_graph.py</a> illustrated in the following code fragment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.contrib.decent_q</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="kn">import</span> <span class="n">gfile</span>

<span class="k">def</span> <span class="nf">softmax_predict</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span> <span class="c1">#DB: added</span>
	<span class="n">prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;prediction&#39;</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">prediction</span>

  <span class="k">def</span> <span class="nf">graph_eval</span><span class="p">(</span><span class="n">input_graph_def</span><span class="p">,</span> <span class="n">input_node</span><span class="p">,</span> <span class="n">output_node</span><span class="p">):</span>
      <span class="c1">#Reading images and segmentation labels</span>
      <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">get_images_and_labels</span><span class="p">(</span><span class="n">IMG_TEST_DIR</span><span class="p">,</span> <span class="n">SEG_TEST_DIR</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">WIDTH</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">HEIGHT</span><span class="p">)</span>

      <span class="c1"># load graph</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">import_graph_def</span><span class="p">(</span><span class="n">input_graph_def</span><span class="p">,</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>

      <span class="c1"># Get input &amp; output tensors</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="n">input_node</span><span class="o">+</span><span class="s1">&#39;:0&#39;</span><span class="p">)</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="n">output_node</span><span class="o">+</span><span class="s1">&#39;:0&#39;</span><span class="p">)</span>

      <span class="c1"># Create the Computational graph</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

          <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">global_variables</span><span class="p">())</span>

          <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">x_test</span><span class="p">}</span>
          <span class="c1">#y_pred = sess.run(y, feed_dict) # original code</span>

          <span class="n">logits</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">)</span>  <span class="c1"># new code</span>
          <span class="n">pred_DB</span> <span class="o">=</span> <span class="n">softmax_predict</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="c1">#new code</span>
          <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pred_DB</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1">#new code</span>

      <span class="c1"># Calculate intersection over union for each segmentation class</span>
      <span class="n">y_predi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
      <span class="n">y_testi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
      <span class="n">cnn</span><span class="o">.</span><span class="n">IoU</span><span class="p">(</span><span class="n">y_testi</span><span class="p">,</span><span class="n">y_predi</span><span class="p">)</span>
</pre></div>
</div>
<p>These changes are implemented into the new script called <a class="reference external" href="files/code/eval_quantized_graph.py">eval_quantized_graph.py</a>.</p>
<p>Note that the output node names must be different, as illustrated by the below fragment of code in the  <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">OUTPUT_NODE</span><span class="o">=</span><span class="s2">&quot;activation_1/truediv&quot;</span> <span class="c1"># output node of floating point CNN</span>
<span class="nv">Q_OUTPUT_NODE</span><span class="o">=</span><span class="s2">&quot;conv2d_transpose_2/conv2d_transpose&quot;</span> <span class="c1"># output node of quantized CNN</span>
</pre></div>
</div>
<p>This is due to the fact that the <code class="docutils literal notranslate"><span class="pre">Softmax</span></code> classifier layer has to be computed in software by the ARM CPU out of the DPU core.</p>
<p>In order to find the name of the output node, you have to use one of these two tools: either <code class="docutils literal notranslate"><span class="pre">netron</span></code> or <code class="docutils literal notranslate"><span class="pre">tensorboard</span></code>. The first has to be installed from your python virtual environment with the command <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">netron</span></code> (I am using version 3.4.6), the second comes automatically with the TF release inside Vitis AI tools image.
For example, taking FCN8 as reference, run the <a class="reference external" href="files/open_pb_graph_in_tensorBoard.sh">open_pb_graph_in_tensorBoard.sh</a> script with the following command to use <code class="docutils literal notranslate"><span class="pre">tensorboard</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="n">open_pb_graph_in_tensorBoard</span><span class="o">.</span><span class="n">sh</span> <span class="o">./</span><span class="n">workspace</span><span class="o">/</span><span class="n">quantize_results</span><span class="o">/</span><span class="n">fcn8</span><span class="o">/</span><span class="n">quantize_eval_model</span><span class="o">.</span><span class="n">pb</span>
</pre></div>
</div>
<p>and then with a browser you will see what illustrated in Figure 8.
Alternatively you can load the final graph <code class="docutils literal notranslate"><span class="pre">./workspace/quantize_results/fcn8/deploy_model.pb</span></code> directly with <code class="docutils literal notranslate"><span class="pre">netron</span></code> and you will see in your browser what illustrated in Figure 9.</p>
<p><img alt="figure8" src="../../../_images/fcn8_tensorboard.png" /></p>
<p><em>Figure 8: Final part of the FCN8 graph, as it appears in TensorBoard.</em></p>
<p><img alt="figure9" src="../../../_images/fcn8_netron.png" /></p>
<p><em>Figure 9: Final part of FCN8 graph, as it appears in Netron.</em></p>
<p>If you enable the following fragment of code from <a class="reference external" href="files/code/fcn8_make_predictions.py">fcn8_make_predictions.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Visualize the model performance</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">visualize_model_performance</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred1_i</span><span class="p">,</span> <span class="n">y_test1_i</span><span class="p">,</span> <span class="n">N_CLASSES</span><span class="p">,</span> <span class="n">UPSCALE</span><span class="p">)</span>
</pre></div>
</div>
<p>you can visualize the predicted segmented images and so you can note the difference between the ideal (ground truth) segmented image (left) and the prediction from the floating point graph (centre) and from the quantized graph (right) as shown in Figure 10:</p>
<p><img alt="figure10" src="../../../_images/eval_frozen_q_example10.png" /></p>
<p><em>Figure 10: Segmentation comparison between ground truth (left), floating point model (centre) and quantized model (roght).</em></p>
<p>You will realize that the prediction is not really completely accurate and suffers a lot of “blocking” artifacts, due to the last layer which does an upscale of 8 in both directions, which is a limitation of FCN8 CNN architecture itself.</p>
</div>
<div class="section" id="compile-the-quantized-models">
<h2>4.6 Compile the Quantized Models<a class="headerlink" href="#compile-the-quantized-models" title="Permalink to this headline">¶</a></h2>
<p>The subroutine <code class="docutils literal notranslate"><span class="pre">6_compile_vai_zcu102()</span></code>  within the <a class="reference external" href="files/run_fcn8.sh">run_fcn8.sh</a> script generates the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file for the embedded system composed by the ARM CPU and the DPU accelerator in the ZCU102 board.</p>
<p>This file has to be loaded by the C++ application file <a class="reference external" href="files/target_zcu102/code/src/main.cc">main.cc</a> at runtime, for example, in case of FCN8 and FCN8UPS the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> files are named respectively <code class="docutils literal notranslate"><span class="pre">fcn8.xmodel</span></code> and <code class="docutils literal notranslate"><span class="pre">fcn8ups.xmodel</span></code>.</p>
<p>If you use the Python VART APIs instead of the C++ APIs, to interact with the DPU core the first lines of the script <a class="reference external" href="files/target_zcu102/code/src/app_mt.py">app_mt.py</a>  must contain:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">vart</span>
<span class="kn">import</span> <span class="nn">xir</span>
</pre></div>
</div>
</div>
<div class="section" id="build-and-run-on-zcu102-target-board">
<h2>4.7 Build and Run on ZCU102 Target Board<a class="headerlink" href="#build-and-run-on-zcu102-target-board" title="Permalink to this headline">¶</a></h2>
<p>In this design, you will use C++ to measure the performance in terms of fps and the Python APIs to get the prediction accuracy.</p>
<p>You have to cross-compile the hybrid (CPU + DPU) application from the host side (out of the docker tools image) with <a class="reference external" href="files/target_zcu102/code/build_app.sh">build_app.sh</a> shell script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">unset</span> LD_LIBRARY_PATH   
sh ~/petalinux_sdk/2020.2/environment-setup-aarch64-xilinx-linux <span class="c1"># set petalinux environment of Vitis AI 1.3</span>
<span class="nb">cd</span> &lt;WRK_DIR&gt;/tutorials/VAI-KERAS-FCN8-SEMSEG/files
<span class="nb">cd</span> target_zcu102/code
bash -x ./build_app.sh
<span class="nb">cd</span> ..
tar -cvf target_zcu102.tar ./target_zcu102 <span class="c1"># to be copied on the SD card</span>
</pre></div>
</div>
<p>Note that a subset of the <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> environment is also available directly on the SD card target board, so you can compile the application directly from there. In fact this is what the script <code class="docutils literal notranslate"><span class="pre">run_all_target.sh</span></code> indeed does, once you will launch it from the target board.</p>
<p>Assuming you have transferred the <code class="docutils literal notranslate"><span class="pre">target_zcu102.tar</span></code> archive from the host to the target board with the <code class="docutils literal notranslate"><span class="pre">scp</span></code> utility, you can now run the following command directly on the target board:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar -xvf target_zcu102.tar
<span class="nb">cd</span> target_zcu102
bash ./run_all_target.sh
</pre></div>
</div>
<p>The purpose of <a class="reference external" href="files/target_zcu102/run_all_target.sh">run_all_target.sh</a> shell script is</p>
<ol class="simple">
<li><p>to extract the test images from the <code class="docutils literal notranslate"><span class="pre">test.tar.gz</span></code>  archive;</p></li>
<li><p>to launch the <code class="docutils literal notranslate"><span class="pre">app_mt.py</span></code> application based on VART Python APIs and measure the effective fps performance at run time;</p></li>
<li><p>to run the C++ executable in order of creating images with segmentation results overlapped.
Note that the code to compute the effective <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> prediction accuracy is still missing, but you should be easily derive it from  the python scripts of the training process.</p></li>
</ol>
</div>
</div>
<div class="section" id="accuracy-performance-results">
<h1>5 Accuracy &amp; Performance results<a class="headerlink" href="#accuracy-performance-results" title="Permalink to this headline">¶</a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> prediction accuracy evaluations of the floating-point and of the quantized frozen graphs can be compared to the INT8 post-quantization model and actual results obtained by the hardware model running on the ZCU102 board:</p>
<p>| Floating point  |  quantized model      | Hardware model (INT8) |
| :————-: | :——————-: | :——————-: |
|        0.383     |        0.375         |          0.373        |</p>
<p>The approximate throughput (in frames/sec) for increasing number of threads is shown below:</p>
<p>| Threads   | Throughput (fps) |
| :——-: | :————–: |
|     1     |      21.62       |
|     2     |      41.09       |
|     4     |      55.53       |
|     6     |      55.28       |</p>
</div>
<div class="section" id="summary">
<h1>6 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial you have seen how to implement on the Xilinx <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">ZCU102</a> board some CNNs suitable for Semantic Segmentation with a small custom dataset of few hundreds of images taken from <a class="reference external" href="https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view">here</a>.</p>
<p>Taking FCN8 CNN as an example, the Xilinx <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.3</a> has quantized the <code class="docutils literal notranslate"><span class="pre">ep200_trained_fcn8_224x224hdf5</span></code> 32-bit floating point weights file (of ~224MB size) generated by the training process in Keras, into an 8-bit fixed point <code class="docutils literal notranslate"><span class="pre">deploy_model.pb</span></code> file (of ~112MB size) and such <code class="docutils literal notranslate"><span class="pre">pb</span></code> file is then transformed into the <code class="docutils literal notranslate"><span class="pre">fcn8.xmodel</span></code> (of ~65MB size) file for the DPU accelerator.</p>
<p>The advantage of this small dataset is that it makes the training process in Keras short enough, but the <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> prediction accuracy is only ~0.38. To get a larger value you probably need a larger dataset, as the <a class="reference external" href="http://cocodataset.org/#home">MS COCO</a> or the <a class="reference external" href="https://www.cityscapes-dataset.com/">Cityscapes</a>, although this would probably need to re-architect the deepness of FCN8 to make it suitable to images of size 1920x1080, instead of 224x224 as in this case study.</p>
<p>Despite that, you have seen how easy is to control the DPU core from the embedded Linux Ubuntu OS on the ZC102 board via the <strong>DPU Python VART APIs</strong> with the <a class="reference external" href="files/target_zcu102/code/src/app_mt.py">app_mt.py</a> script. The traditional C++ programming of the embedded system composed by the ARM CPU and the DPU accelerator is also available in the <a class="reference external" href="files/target_zcu102/code/src/main.cc">main.cc</a> application file.</p>
</div>
<div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<div class="section" id="a1-semantic-segmentation-background">
<h2>A1 Semantic Segmentation Background<a class="headerlink" href="#a1-semantic-segmentation-background" title="Permalink to this headline">¶</a></h2>
<p><strong>Image classification</strong> is the task of predicting labels or categories. <strong>Object detection</strong> is the task
of predicting bounding boxes: each bounding box may have objects other than the detected object inside it.
<strong>Semantic segmentation</strong> is the task of predicting pixelwise labels: for example the labels can be the sky, trees, persons, mountains, etc and are assigned to each pixel.
The task of segmenting every instance with a different pixel-wise label (for example each person in a picture is labeled with a different value) is called <strong>instance segmentation</strong>.</p>
<p>The most adopted datasets in the latest years are <a class="reference external" href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/">PASCAL</a>, <a class="reference external" href="https://www.cityscapes-dataset.com/">Cityscapes</a> and <a class="reference external" href="http://cocodataset.org/#home">MS COCO</a>. New algorithms are usually benchmarked against the MS COCO dataset. Creating training data for semantic segmentation is expensive as it requires proper CV tools for annotating the datasets.</p>
<p>Usually the decoded image is of the same size of the input.</p>
<p>Each pixel of the output of the CNN is compared with the corresponding pixel in the <strong>ground truth</strong> segmentation image (also called <strong>masks</strong>). Standard <strong>cross entropy loss</strong> is computed on each pixel. For the segmentation masks do not use <strong>jpg</strong> format being it lossy, use <strong>bmp</strong> or <strong>png</strong> instead.</p>
<p>The most adopted CNNs for semantic segmentation are:</p>
<ul class="simple">
<li><p><strong>FCN</strong> (Fully Convolutional Network): it is an Image classification CNN in which neither Max-Pooling nor Fully-Connected (FC) layers are used. For many applications, choosing a model pre-trained on ImageNet is the best choice, similarly to Transfer Learning, for example VGG could be converted to a FCN by making the last FC layers 1x1 Convolutions.  The three variants are FCN8, FCN16 and FCN32. In FCN8 and FCN16, <strong>skip connections</strong> architecture is also adopted.</p></li>
<li><p><strong>SegNet</strong>: tries to improve the FCN coarse outputs by using an encoder-decoder approach, with the encoder and decoder layers symmetrical to each other. The features are reduced in dimensions in the encoder stage via <strong>dilated convolution</strong> - also called <strong>strided convolution</strong> (it is a convolution with <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;=2</span></code>) - and then upsampled again in the decoder stage with either the  <strong>transposed convolution</strong> - also called <strong>deconvolution</strong> or <strong>upconvolution</strong> - or the <strong>Max Unpooling</strong> (which uses positions from pooling layers).
SegNet does not applies skip connections.</p></li>
<li><p><strong>UNet</strong>: this architecture adopts an encoder-decoder framework with skip connections. Like SegNet, the encoder and decoder layers are symmetrical to each other.</p></li>
<li><p><strong>PSPNet</strong>: The Pyramid Scene Parsing Network is optimized to learn better global context representation of a scene. First, the image is passed to the base network to get a feature map. The the feature map is downsampled to different scales. Convolution is applied to the pooled feature maps. After that, all the feature maps are upsampled to a common scale and concatenated together. Finally a another convolution layer is used to produce the final segmentation outputs. Here, the smaller objects are captured well by the features pooled to a high resolution, whereas the large objects are captured by the features pooled to a smaller size.</p></li>
</ul>
<p>Most of this section was based on R. Shanmugamani’s “Deep Learning for Computer Vision (Expert techniques to train advanced neural networks using TensorFlow and Keras)” book, 2018 Packt Publishing.</p>
<div class="section" id="a1-1-intersection-over-union-iou">
<h3>A1.1 Intersection over Union (IoU)<a class="headerlink" href="#a1-1-intersection-over-union-iou" title="Permalink to this headline">¶</a></h3>
<p>Both Semantic Segmentation and Object Detection measure the prediction accuracy in terms of “Intersection over Union” or <code class="docutils literal notranslate"><span class="pre">IoU</span></code>. You can read this nice <a class="reference external" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">IoU PyImageSearch tutorial</a> to learn more about it.</p>
</div>
<div class="section" id="a1-2-ms-coco-dataset">
<h3>A1.2 MS COCO dataset<a class="headerlink" href="#a1-2-ms-coco-dataset" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="http://cocodataset.org/#home">MS COCO</a> dataset contains 91 object categories with 82 of them having more than 5000 labeled instances, as explained in this paper titled <a class="reference external" href="https://arxiv.org/pdf/1405.0312.pdf">MicroSoft COCO: Common Objects in COntext</a>.</p>
<p>The datasets can be downloaded <a class="reference external" href="http://cocodataset.org/#download">here</a>. The <a class="reference external" href="https://github.com/cocodataset/cocoapi">COCO API</a> are available both in MATLAB and Python.</p>
</div>
<div class="section" id="a1-3-reference-tutorials">
<h3>A1.3 Reference tutorials<a class="headerlink" href="#a1-3-reference-tutorials" title="Permalink to this headline">¶</a></h3>
<p>There are some nice tutorials on Semantic Segmentation with Keras/TensorFlow, here are my preferred ones:</p>
<ol class="simple">
<li><p><a class="reference external" href="https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html">A Beginner’s guide to Deep Learning based Semantic Segmentation using Keras</a> and related <a class="reference external" href="https://github.com/divamgupta/image-segmentation-keras">GitHub project files</a></p></li>
<li><p>PyImageSearch’ <a class="reference external" href="https://www.pyimagesearch.com/2018/09/03/semantic-segmentation-with-opencv-and-deep-learning/">Semantic segmentation with OpenCV and deep learning</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/a-keras-pipeline-for-image-segmentation-part-1-6515a421157d">A Keras Pipeline for Image Segmentation</a></p></li>
<li><p><a class="reference external" href="https://www.microsoft.com/developerblog/2018/07/18/semantic-segmentation-small-data-using-keras-azure-deep-learning-virtual-machine/">Semantic Segmentation of Small Data using Keras on an Azure Deep Learning Virtual Machine</a></p></li>
<li><p><a class="reference external" href="https://missinglink.ai/guides/deep-learning-frameworks/tensorflow-image-segmentation-two-quick-tutorials/">TensorFlow Image Segmentation: Two Quick Tutorials</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;hanrelan/a-non-experts-guide-to-image-segmentation-using-deep-neural-nets-dda5022f6282">A Non-Expert’s Guide to Image Segmentation Using Deep Neural Nets</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;daj/how-to-inspect-a-pre-trained-tensorflow-model-5fd2ee79ced0">How to inspect a pre-trained TensorFlow model</a></p></li>
<li><p><a class="reference external" href="http://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch">http://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch</a></p></li>
<li><p><a class="reference external" href="https://www.udemy.com/creating-coco-datasets/?couponCode=SUPPORTER">https://www.udemy.com/creating-coco-datasets/?couponCode=SUPPORTER</a></p></li>
<li><p><a class="reference external" href="https://commecica.com/2018/07/18/starting-with-darknet-yolo-coco/">https://commecica.com/2018/07/18/starting-with-darknet-yolo-coco/</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=h6s61a_pqfM">https://www.youtube.com/watch?v=h6s61a_pqfM</a></p></li>
<li><p><a class="reference external" href="https://gluon-cv.mxnet.io/build/examples_datasets/mscoco.html">https://gluon-cv.mxnet.io/build/examples_datasets/mscoco.html</a></p></li>
<li><p><a class="reference external" href="https://github.com/nightrome/cocostuff#setup">https://github.com/nightrome/cocostuff#setup</a></p></li>
<li><p><a class="reference external" href="http://deeplearning.net/tutorial/fcn_2D_segm.html">Fully Convolutional Networks (FCN) for 2D segmentation</a></p></li>
<li><p><a class="reference external" href="https://fairyonice.github.io/Learn-about-Fully-Convolutional-Networks-for-semantic-segmentation.html">Learn about Fully Convolutional Networks for semantic segmentation</a></p></li>
<li><p><a class="reference external" href="https://seaborn.pydata.org/tutorial.html">Official seaborn tutorial</a></p></li>
<li><p><a class="reference external" href="https://www.jeremyjordan.me/evaluating-image-segmentation-models/">Evaluating Image Segmentation Models</a></p></li>
<li><p><a class="reference external" href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/">Motion-based Segmentation and Recognition Dataset</a></p></li>
</ol>
</div>
</div>
<div class="section" id="a2-build-and-run-on-vck190-target-board">
<h2>A2 Build and Run on VCK190 Target Board<a class="headerlink" href="#a2-build-and-run-on-vck190-target-board" title="Permalink to this headline">¶</a></h2>
<p>Alternatively to ZCU102, you can also use the <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/vck190.html">VCK190</a> and its  following files, available only from the <a class="reference external" href="https://www.xilinx.com/member/versal-ml-ea.html#versal-dpu">Versal ML EA Lounge</a> (you need to be registered to it to get access to its content):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>permissions | size (bytes) | filename
-rwxrwxrwx  |       2397   | readme.txt
-rwxrwxrwx  |      14868   | VCK190 Network Support List.xlsx
-rwxrwxrwx  | 2264655516   | xilinx_model_zoo_vck190-1.3.0-r200-Linux.tar.gz
-rwxrwxrwx  |  372211940   | xilinx-vck190-dpu-v2020.1-v1.3.0.img.gz
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> files generated for VCK190 are necessarily different from the ones of ZCU102, because the DPU architecture of the first board is different from the DPU of the second board. No changes to the C++ or Python files are needed for these four CNN examples.</p>
<p>Working with VCK190 board requires just to adopt the <code class="docutils literal notranslate"><span class="pre">6_compile_vai_vck190()</span></code> routine from the script <a class="reference external" href="files/%5Brun_fcn8.sh">run_fcn8.sh</a>, instead of the <code class="docutils literal notranslate"><span class="pre">6_compile_vai_zcu102()</span></code> which is related to ZCU102.</p>
<p>Make a <code class="docutils literal notranslate"><span class="pre">tar</span></code> file of the <code class="docutils literal notranslate"><span class="pre">target_vck190</span></code>  folder, copy it from the host PC to the target ZCU104 board. For example, in case of an Ubuntu PC, use the following command (assuming the board IP address is always the same):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">target_vck190</span><span class="o">.</span><span class="n">tar</span> <span class="n">root</span><span class="o">@</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.100</span><span class="p">:</span><span class="o">/</span><span class="n">root</span><span class="o">/</span>
<span class="n">cd</span> <span class="n">target_vck190</span>
<span class="n">source</span> <span class="o">./</span><span class="n">run_all_target</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">IoU</span></code> prediction accuracy evaluations of the floating-point and of the quantized frozen graphs can be compared to the INT8 post-quantization model and actual results obtained by the hardware model running on the ZCU102 board:</p>
<p>| Floating point  |  quantized model      | Hardware model (INT8) |
| :————-: | :——————-: | :——————-: |
|        0.383     |        0.375         |        0.373         |</p>
<p>The approximate throughput (in frames/sec) for increasing number of threads is shown below:</p>
<p>| Threads   | Throughput (fps) |
| :——-: | :————–: |
|     1     |      151.62      |
|     2     |      278.23      |
|     4     |      313.88      |
|     6     |      304.60      |</p>
<p>Note that FCN8UPS and UNET CNNs do not work as the <code class="docutils literal notranslate"><span class="pre">depth-wise</span> <span class="pre">conv</span></code> layer  is not yet supported on the Versal DPU. Alternatively the CNN should be split into more sub-graphs, with the unsupported layer implemented in a subgraph running on the ARM CPU.</p>
<hr/>
<p align="center"><sup>Copyright&copy; 2019 Xilinx</sup></p></div>
</div>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
															</div>
														</div>
														<div class="col-md-pull-6 col-lg-pull-6 col-md-6 col-lg-6">
															<span class="copyright">
                                  
                                  &copy; 2020–2021, Xilinx, Inc.
                              </span>
															<ul class="list-inline sub-menu">
																<li>
																	<a href="https://www.xilinx.com/about/privacy-policy.html">Privacy</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/legal.html">Legal</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/contact.html">Contact</a>
																</li>
															</ul>
														</div>
													</div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="quicklinks parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<noindex>
						<span class="quickLinks">
							<ul>
								<li>
									<a href="#top" class="btn backToTop">
									<span class="fas fa-angle-up" aria-hidden="true"></span>
									</a>
								</li>
							</ul>
						</span>
					</noindex>
				</div>
			</div>
		</div>
		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>