


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		
		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

  
  
  
  

  
      <script type="text/javascript" src="../../../_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="../../../_static/js/gtm.js"></script>
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="../../../_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/header-footer.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/linkid.js"></script>
    <script type="text/javascript" src="../../../_static/js/Searchbox.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pro.min.css" media="all" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="header parbase aem-GridColumn aem-GridColumn--default--12">
								<noindex>
									<header data-component="header">
										<nav class="navbar navbar-default aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid main-nav">
													<div class="row">
														<div class="col-xs-12">
															<div class="logo-column">
																<div class="logo">
																	<a href="https://www.xilinx.com/">
																	<img src="https://www.xilinx.com/etc.clientlibs/site/clientlibs/xilinx/all/resources/imgs/header/xilinx-header-logo.svg" title="Xilinx Inc"/>
																	</a>
																</div>
															</div>
															<div class="navbar-column">
																<div class="navbar navbar-collapse collapse" id="xilinx-main-menu">
																	<div class="mobile-search-container">
																		<div id="headerSearchBox" class="headerSearch"
																			data-component="header-search"
																			data-redirect-if-empty="false"
																			data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																			data-coveo-organization-id="xilinxcomprode2rjoqok">
																			<div class='coveo-search-section'>
																				<div class="CoveoAnalytics" data-search-hub="Site"></div>
																				<ul class="dropdown-menu options">
																					<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																						<a href="#">
																						All</a>
																					</li>
																					<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com//products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Silicon Devices</a>
																					</li>
																					<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com//products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Boards and Kits</a>
																					</li>
																					<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com//products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Intellectual Property</a>
																					</li>
																					<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																						<a href="#">
																						Support</a>
																						<ul>
																							<li data-label="Documentation" data-action-link="https://www.xilinx.com//support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																								<a href="#">
																								Documentation</a>
																							</li>
																							<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com//support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																								<a href="#">
																								Knowledge Base</a>
																							</li>
																							<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																								<a href="#">
																								Community Forums</a>
																							</li>
																						</ul>
																					</li>
																					<li data-label="Partners" data-action-link="https://www.xilinx.com//alliance/member-keyword-search.html" data-search-hub="Partner">
																						<a href="#">
																						Partners</a>
																					</li>
																					<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																						<a href="#">
																						Videos</a>
																					</li>
																					<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																						<a href="#">
																						Press</a>
																					</li>
																				</ul>
																				<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																				<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																			</div>
																		</div>
																	</div>
																	<ul class="nav navbar-nav nav-justified">
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/applications.html">
																			Applications</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/products/silicon-devices.html">
																			Products</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://developer.xilinx.com/">
																			Developers</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/support.html">
																			Support</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/about/company-overview.html">
																			About</a>
																		</li>
																	</ul>
																</div>
															</div>
															<script type="text/javascript" src="../../../_static/js/gtm.js"></script>
															<!--<div class="mini-nav">
																<button type="button" data-function="xilinx-mobile-menu" id="nav-toggle" class="navbar-toggle collapsed visible-xs-block" aria-expanded="false">
																<span></span>
																<span></span>
																<span></span>
																<span></span>
																</button>
																<ul class="list-inline">
																	<li class="dropdown user-menu">
																		<button data-toggle="dropdown">
																		<span class="sr-only">Account</span>
																		<span class="fas fa-user"></span>
																		</button>
																		<ul class="dropdown-menu">
																			<li>
																				<a href="https://www.xilinx.com/myprofile/subscriptions.html">
																				My Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/registration/create-account.html">
																				Create Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/bin/protected/en/signout">
																				Sign Out</a>
																			</li>
																		</ul>
																	</li>
																	<li class="hidden-xs">
																		<button data-function="search-toggle">
																		<span class="sr-only">Search</span>
																		<span class="far fa-search"></span>
																		</button>
																	</li>
																</ul>
															</div>
															-->
															<div class="search-container">
																<div id="headerSearchBox" class="headerSearch"
																	data-component="header-search"
																	data-redirect-if-empty="false"
																	data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																	data-coveo-organization-id="xilinxcomprode2rjoqok">
																	<div class='coveo-search-section'>
																		<div class="CoveoAnalytics" data-search-hub="Site"></div>
																		<ul class="dropdown-menu options">
																			<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																				<a href="#">
																				All</a>
																			</li>
																			<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com/products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Silicon Devices</a>
																			</li>
																			<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com/products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Boards and Kits</a>
																			</li>
																			<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com/products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Intellectual Property</a>
																			</li>
																			<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																				<a href="#">
																				Support</a>
																				<ul>
																					<li data-label="Documentation" data-action-link="https://www.xilinx.com/support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																						<a href="#">
																						Documentation</a>
																					</li>
																					<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com/support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																						<a href="#">
																						Knowledge Base</a>
																					</li>
																					<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																						<a href="#">
																						Community Forums</a>
																					</li>
																				</ul>
																			</li>
																			<li data-label="Partners" data-action-link="https://www.xilinx.com/alliance/member-keyword-search.html" data-search-hub="Partner">
																				<a href="#">
																				Partners</a>
																			</li>
																			<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																				<a href="#">
																				Videos</a>
																			</li>
																			<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																				<a href="#">
																				Press</a>
																			</li>
																		</ul>
																		<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																		<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																	</div>
																</div>
																<button data-function="search-toggle">
																<span class="sr-only">Search</span>
																<span class="far fa-times"></span>
																</button>
															</div>
														</div>
													</div>
												</div>
											</div>
										</nav>
									</header>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Vitis チュートリアル
          

          
          </a>

          
            
            
              <div class="version">
                2020.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <p class="caption"><span class="caption-text">English version</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs/index.html">Master</a></li>
</ul>
<p class="caption"><span class="caption-text">入門</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis/README.html">Vitis フロー 101 チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis_HLS/README.html">Vitis HLS の解析および最適化</a></li>
</ul>
<p class="caption"><span class="caption-text">機械学習 (英語版)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Introduction to Machine Learning with Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">アクセラレーション</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html">Vitis ハードウェア アクセラレータの概要</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#id1">設計チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#id2">機能チュートリアル</a></li>
</ul>
<p class="caption"><span class="caption-text">AI エンジン開発 (英語版)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/README.html">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">プラットフォーム作成チュートリアル</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Vitis_Platform_Creation/README.html">プラットフォームの作成</a></li>
</ul>
<p class="caption"><span class="caption-text">XRT および Vitis システム最適化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Runtime_and_System_Optimization/README.html">設計チュートリアル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Runtime_and_System_Optimization/README.html#id2">機能チュートリアル</a></li>
</ul>
<p class="caption"><span class="caption-text">バージョン</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-1/docs/README.html">2020.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-Tutorials/blob/Vitis-Tutorials-2019.2-Hotfix1/README.md">2019.2</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="../../../_sources/Machine_Learning/Design_Tutorials/01-caffe_cats_vs_dogs/README.md.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Vitis チュートリアル</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Current status</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Machine_Learning/Design_Tutorials/01-caffe_cats_vs_dogs/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <table>
 <tr>
   <td align="center"><img src="https://www.xilinx.com/content/dam/xilinx/imgs/press/media-kits/corporate/xilinx-logo.png" width="30%"/><h1>Vitis AI Tutorials</h1>
   </td>
 </tr>
 <tr>
 <td align="center"><h3> Quantization and Pruning of AlexNet CNN trained in Caffe with Cats-vs-Dogs dataset</h3>
 </td>
 </tr>
</table><div class="section" id="current-status">
<h1>Current status<a class="headerlink" href="#current-status" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p>Tested with Vitis AI 1.3.</p></li>
<li><p>Tested in hardware on ZCU102 board.</p></li>
<li><p>Totally replaced old DNNDK APIs with VART APIs.</p></li>
</ol>
<p>** Date: 13 Jan 2021 **</p>
</div>
<div class="section" id="introduction">
<h1>1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>This Deep Learning (DL) tutorial shows you how to train, prune, and quantize a modified version of the AlexNet convolutional neural network (CNN) with the Kaggle <a class="reference external" href="https://www.kaggle.com/c/dogs-vs-cats">Dogs vs. Cats</a> dataset in order to deploy it on the Xilinx® <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">ZCU102</a> board. You will use the <a class="reference external" href="http://caffe.berkeleyvision.org">Caffe</a> environment available from <a class="reference external" href="https://developer.xilinx.com/en/get-started/ai.html">Vitis AI</a>, which is a set of optimized IP, tools libraries, models and example designs valid for AI inference on both Xilinx edge devices and Alveo cards.</p>
<p>Once the selected CNN has been correctly trained in Caffe, the <code class="docutils literal notranslate"><span class="pre">caffemodel</span></code> file of floating point weights is then quantized by the Vitis AI Quantizer that creates an 8-bit integer representation (named “INT8”) from which the Vitis AI Compiler generates the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file of micro instructions for the Deep Processor Unit (DPU). The final application is executed at run time - via VART C++ APIs - on the target board to make predictions that can be compared against the simulation reference results.</p>
<p>The Dogs vs. Cats dataset is composed of only two classes of objects to be classified: cats and dogs. It contains 25000 RGB images that have been organized into the following three databases (all the images are randomly shuffled before forming the database):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train_lmdb</span></code>: 20000 images (resized to 256x256) in the LMDB database for the forward/backward training process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">valid_lmdb</span></code>:  4000 images (resized to 256x256) in the LMDB database for the validation step during training process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test</span></code>: 1000 images (resized to 227x227) in plain JPEG format for the top-1 prediction measurements, once the CNN is trained.</p></li>
</ul>
<p>AlexNet is a well-known CNN that works with images 227x227x3 in size. It is described in the
<a class="reference external" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book">Practitioner Bundle book</a> by <a class="reference external" href="https://www.linkedin.com/in/adrian-rosebrock-59b8732a/">Dr. Adrian Rosebrock</a> from <a class="reference external" href="https://www.pyimagesearch.com/">PyImageSearch</a> where it is modeled and trained in Keras/TensorFlow. The model adopted in this tutorial was manually translated into <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> files and trained with Caffe from scratch. Moreover some layers were organized differently:</p>
<ul class="simple">
<li><p>“Local Response Normalization” (LRN) layers were replaced by “Batch Normalization” (BN) layers;</p></li>
<li><p>the “ReLU” activation layer was placed after the BN layer, and not before;</p></li>
<li><p>the number of BN and “DROPOUT” layers were also reduced.</p></li>
</ul>
<p>:pushpin: <strong>Note:</strong> The Xilinx <a class="reference external" href="https://www.xilinx.com/cgi-bin/docs/rdoc?t=vitis_ai%3Bv=1.3%3Bd=ug1333-ai-optimizer.pdf">Vitis AI Optimizer</a>, as known as “pruning”, requires a license fee and can be accessed only at its <a class="reference external" href="https://www.xilinx.com/member/ai_optimizer.html">lounge</a> page. Therefore the pruning tool is not included in this tutorial, although all the shell scripts to prune the CNN and related log files are available in the <a class="reference external" href="files/pruning/alexnetBNnoLRN/">pruning</a> folder.</p>
</div>
<div class="section" id="prerequisites">
<h1>2 Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Ubuntu 16.04 host PC with Python 3.6.</p></li>
<li><p>The entire repository of <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.3</a> from <a class="reference external" href="https://www.github.com/Xilinx">www.github.com/Xilinx</a>.</p></li>
<li><p>Accurate reading of <a class="reference external" href="https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf">Vitis AI User Guide UG1414 v1.3</a>. In particular:</p></li>
</ul>
<ol class="simple">
<li><p>“Vitis AI Overview” in Chapter 1 with DPU naming and guidelines to download the tools container available from <a class="reference external" href="https://hub.docker.com/r/xilinx/vitis-ai/tags">docker hub</a> and the Runtime Package for edge (MPSoC) devices.</p></li>
<li><p>“Installation and Setup” instructions of Chapter 2 for both host and target;</p></li>
<li><p>“Quantizing the Model” in Chapter 4 and “Compiling the Model” in Chapter 5.</p></li>
<li><p>“Programming with VART” APIs in Chapter 6.</p></li>
</ol>
<ul class="simple">
<li><p>A Vitis AI Evaluation board such as the <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">ZCU102</a> with its <a class="reference external" href="https://www.xilinx.com/bin/public/openDownload?filename=xilinx-zcu102-dpu-v2020.2-v1.3.0.img.gz">image file</a>, which contains a pre-built working design for the ZCU102 with the DPUCZDX8G (renamed shortly as “DPUv2” in the following).</p></li>
<li><p>Familiarity with Deep Learning principles.</p></li>
<li><p>Familiarity with Caffe (here is the official <a class="reference external" href="http://caffe.berkeleyvision.org/tutorial/">online Caffe tutorial</a>).</p></li>
</ul>
<div class="section" id="dos-to-unix-conversion">
<h2>Dos-to-Unix Conversion<a class="headerlink" href="#dos-to-unix-conversion" title="Permalink to this headline">¶</a></h2>
<p>In case you might get some strange errors during the execution of the scripts, you have to pre-process -just once- all the<code class="docutils literal notranslate"><span class="pre">*.sh</span></code> shell and the python <code class="docutils literal notranslate"><span class="pre">*.py</span></code> scripts with the <a class="reference external" href="http://archive.ubuntu.com/ubuntu/pool/universe/d/dos2unix/dos2unix_6.0.4.orig.tar.gz">dos2unix</a> utility.
In that case run the following commands from your Ubuntu host PC (out of the Vitis AI docker images):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo apt-get install dos2unix
<span class="nb">cd</span> &lt;WRK_DIR&gt; <span class="c1">#your working directory</span>
<span class="k">for</span> file in <span class="k">$(</span>find . -name <span class="s2">&quot;*.sh&quot;</span><span class="k">)</span><span class="p">;</span> <span class="k">do</span>
  dos2unix <span class="si">${</span><span class="nv">file</span><span class="si">}</span>
<span class="k">done</span>
</pre></div>
</div>
</div>
<div class="section" id="vitis-ai-1-2">
<h2>Vitis AI 1.2<a class="headerlink" href="#vitis-ai-1-2" title="Permalink to this headline">¶</a></h2>
<p>If you need to use the older Vitis AI 1.2 release, just replace this <code class="docutils literal notranslate"><span class="pre">README.md</span></code> file with the one placed in the subfolder
<code class="docutils literal notranslate"><span class="pre">vai_1v2</span></code> and go on in following the instructions on that file and the related <code class="docutils literal notranslate"><span class="pre">vai_1v2.zip</span></code> archive, then skip the rest of this document.</p>
</div>
</div>
<div class="section" id="before-starting-with-vitis-ai-1-3">
<h1>3 Before starting with Vitis AI 1.3<a class="headerlink" href="#before-starting-with-vitis-ai-1-3" title="Permalink to this headline">¶</a></h1>
<p>You have to know few things about <a class="reference external" href="https://docs.docker.com/">Docker</a> in order to run the Vitis AI smoothly.</p>
<p>In the following of this document, it is assumed that you have cloned the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.3</a>  (for example in a folder renamed <code class="docutils literal notranslate"><span class="pre">~/ML/VAI1v3</span></code>, as in my case), which becomes now your working directory <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code>.</p>
<p>To list the currently available docker images run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker images <span class="c1"># to list the current docker images available in the host pc</span>
</pre></div>
</div>
<p>and you should see something like in the following text:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>REPOSITORY            TAG                               IMAGE ID            CREATED             SIZE
xilinx/vitis-ai-gpu   1.3                               0b5e7cc1bef5        3 weeks ago         27.5GB
</pre></div>
</div>
<p>To launch the docker container with Vitis AI tools - to do all the steps from CNN training to generation of the ELF file for the DPU - based on CPU (or GPU), execute the following commands from the <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt; <span class="c1"># you are now in Vitis_AI subfolder</span>
./docker_run.sh xilinx/vitis-ai-gpu:1.3
</pre></div>
</div>
<p>Note that the container maps the shared folder <code class="docutils literal notranslate"><span class="pre">/workspace</span></code> with the file system of the Host PC from where you launch the above command, which is <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> in your case.
This shared folder enables you to transfer files from the Host PC to the docker container and vice versa.</p>
<p>The docker container do not have any graphic editor, so it is recommended that you work with two terminals and you point to the same folder, in one terminal you use the docker container commands and in the other terminal you open any graphic editor you like.</p>
<p>Note that docker does not have an automatic garbage collection system as of now. You can use this command to do a manual garbage collection:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker rmi -f $(docker images -f &quot;dangling=true&quot; -q)
</pre></div>
</div>
<p>Starting from Vitis AI 1.1 release there is no more Docker Runtime Container, and you can cross compile the application files directly from the Xilinx <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> environment on your host PC to the target board.
In the following of this tutorial it is assumed that <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> is installed into <code class="docutils literal notranslate"><span class="pre">~/petalinux_sdk</span></code> of your host PC, as recommended in <a class="reference external" href="https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf">UG1414</a>.</p>
<div class="section" id="install-missing-packages-on-the-vitis-ai-tools-container">
<h2>3.1 Install Missing Packages on the Vitis AI Tools Container<a class="headerlink" href="#install-missing-packages-on-the-vitis-ai-tools-container" title="Permalink to this headline">¶</a></h2>
<p>This tutorial requires some packages that were not included in the original Vitis AI tools container. Here are the commands to include such packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./docker_run.sh xilinx/vitis-ai-gpu:1.3    
sudo su <span class="c1"># you must be root</span>
conda activate vitis-ai-caffe     <span class="c1"># as root, enter into Caffe (anaconda-based) virtual environment</span>
<span class="c1">#conda install pycairo==1.18.2    # for Vitis AI &gt;= 1.0</span>
pip install <span class="nv">lmdb</span><span class="o">==</span><span class="m">0</span>.98            
conda deactivate
<span class="nb">exit</span> <span class="c1"># to exit from root</span>
conda activate vitis-ai-caffe <span class="c1"># as normal user, enter into Caffe (anaconda-based) virtual environment</span>
</pre></div>
</div>
<p>Note that if you exit from the current  Vitis AI tools docker image you will lose all the installed packages, so to save all changes just open a new terminal and run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo docker ps -l <span class="c1"># To get the Docker CONTAINER ID</span>
</pre></div>
</div>
<p>you will see the following text (the container ID might have a different number):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONTAINER ID        IMAGE                        COMMAND                CREATED             STATUS              NAMES
7c9927375b06        xilinx/vitis-ai-gpu:1.3      &quot;/etc/login.sh bash&quot;   30 minutes ago      Up 30 minutes       heuristic_lamport
</pre></div>
</div>
<p>now save the modified docker image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo docker commit -m<span class="s2">&quot;caffe&quot;</span> 7c9927375b06 xilinx/vitis-ai-gpu:1.3
</pre></div>
</div>
<p>Assuming you have renamed this project <code class="docutils literal notranslate"><span class="pre">VAI-Caffe-ML-CATSvsDOGS</span></code> and placed it in the directory named <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;/tutorials/</span></code>, you can launch the modified tools container by running the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt;
./docker_run.sh xilinx/vitis-ai-gpu:1.3
<span class="nb">cd</span> /workspace/tutorials/VAI-Caffe-ML-CATSvsDOGS
conda activate vitis-ai-caffe
</pre></div>
</div>
</div>
<div class="section" id="copy-caffe-xilinx-zip">
<h2>3.2 Copy caffe-xilinx.zip<a class="headerlink" href="#copy-caffe-xilinx-zip" title="Permalink to this headline">¶</a></h2>
<p>Now, enter into the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/master/models/AI-Model-Zoo/caffe-xilinx">AI Model Zoo caffe-xilinx</a> file and copy the entire directory, at the same level of this project directory so that both <code class="docutils literal notranslate"><span class="pre">VAI-Caffe-ML-CATSvsDOGS</span></code> and <code class="docutils literal notranslate"><span class="pre">caffe-xilinx</span></code> repositories are placed in <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;/tutorials/</span></code>, with the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt;
cp -r models/AI-Model-Zoo/caffe-xilinx/ ./tutorials/
</pre></div>
</div>
<p>Leave it as it is, you do not need to compile and build anything.</p>
<p>You should see something as illustrated in the screenshot of Figure 1:</p>
<p><img alt="figure1" src="../../../_images/vai_page.png" /></p>
<p><em>Figure 1: Entering into the Vitis AI tools container (screenshot taken from Vitis AI 1.2 release).</em></p>
<p>You will use only the following python2.7 scripts placed in the folder <code class="docutils literal notranslate"><span class="pre">caffe-xilinx/tools/extra</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">caffe-xilinx/tools/extra/parse_log.py</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">caffe-xilinx/tools/extra/extract_seconds.py</span></code></p></li>
</ul>
<p>Note that you need to modify line number 166 of code <code class="docutils literal notranslate"><span class="pre">parse_log.py</span></code> to make it compatible with python &gt;= 3.6:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="s1">&#39;Wrote </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">output_filename</span>
</pre></div>
</div>
<p>into</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Wrote </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">output_filename</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="project-directory-structure">
<h1>4 Project Directory Structure<a class="headerlink" href="#project-directory-structure" title="Permalink to this headline">¶</a></h1>
<p>In Caffe, <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> files cannot use Linux environmental variables, only relative pathnames. This project therefore assumes the following fixed subdirectories, placed under <code class="docutils literal notranslate"><span class="pre">/workspace/tutorials/VAI-Caffe-ML-CATSvsDOGS/</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">caffe/code</span></code> contains all the Python scripts;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">caffe/models</span></code> contains the solver, training, and deploy<code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> files;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deploy</span></code> contains the files for quantization of either the baseline (<code class="docutils literal notranslate"><span class="pre">quantiz</span></code>) or pruned (<code class="docutils literal notranslate"><span class="pre">pruned</span></code>) CNN, plus the files for ZCU102 run-time execution (<code class="docutils literal notranslate"><span class="pre">zcu102/baseline</span></code>, <code class="docutils literal notranslate"><span class="pre">zcu102/pruned</span></code>, and <code class="docutils literal notranslate"><span class="pre">zcu102/test_images</span></code> respectively);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input</span></code> contains the following:</p>
<ul>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database">LMDB</a> databases for the Caffe phases of training and validation;</p></li>
<li><p>JPEG images for testing the top-1 accuracy;</p></li>
<li><p>other JPEG images for calibration during the quantization process.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="get-the-dogs-vs-cats-dataset">
<h1>5 Get the Dogs vs. Cats Dataset<a class="headerlink" href="#get-the-dogs-vs-cats-dataset" title="Permalink to this headline">¶</a></h1>
<p>The dataset cannot be hosted in this repository because of its large size. To obtain the dataset, you need to register on the <a class="reference external" href="https://www.kaggle.com">Kaggle website</a>, setting up a username and password. The registration procedure involves receiving confirmation codes by email and smartphone.</p>
<p>Once registered, you can download the <a class="reference external" href="https://www.kaggle.com/biaiscience/dogs-vs-cats/download">dogs-vs-cats.zip</a> archive. Unzip it where you like and then move the subfolder <code class="docutils literal notranslate"><span class="pre">train</span></code> inside the folder <code class="docutils literal notranslate"><span class="pre">input</span></code> and rename it as <code class="docutils literal notranslate"><span class="pre">jpg</span></code>, for example with the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR/tutorials/VAI-Caffe-ML-CATSvsDOGS/files <span class="c1">#the zip archive with images is supposed to be here</span>
cp kaggle_dogs_vs_cats.zip ./input
<span class="nb">cd</span> input
unzip kaggle_dogs_vs_cats.zip
<span class="nb">cd</span> kaggle_dogs_vs_cats
mv train ../ <span class="c1">#move this folder directly below &quot;input&quot;</span>
<span class="nb">cd</span> ..
mv train jpg <span class="c1"># rename the folder</span>
rm -r kaggle_dogs_vs_cats* <span class="c1">#remove everything no more useful</span>
</pre></div>
</div>
<p>Note that you do not need to use the original <code class="docutils literal notranslate"><span class="pre">test</span></code> archive, because it does not have labeled images and therefore it is not useful for this tutorial.</p>
</div>
<div class="section" id="python-and-shell-scripts">
<h1>6 Python and Shell Scripts<a class="headerlink" href="#python-and-shell-scripts" title="Permalink to this headline">¶</a></h1>
<p>Once the JPEG inages has been put in the folder <code class="docutils literal notranslate"><span class="pre">input/jpg</span></code>, as explained in the previous section, the entire flow can be launched with the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR/tutorials/VAI-Caffe-ML-CATSvsDOGS/files
<span class="nb">source</span> ./run_all_alexnet.sh <span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee logfile_baseline_alexnet_host.txt
</pre></div>
</div>
<p>The shell script <a class="reference external" href="files/run_all_alexnet.sh">run_all_alexnet.sh</a> does all the job for the AlexNet CNN in this order:</p>
<ul class="simple">
<li><p>it sets the environmental variables required by this project by calling  <a class="reference external" href="files/caffe/set_prj_env_variables.sh">set_prj_env_variables.sh</a>; note that this script must be called mandatory prior to any other of the remaining scripts;</p></li>
<li><p>it sets the project directories by launching  <a class="reference external" href="files/set_the_CATSvsDOGS_prj.py">set_the_CATSvsDOGS_prj.py</a> that organizes the images taken from the <code class="docutils literal notranslate"><span class="pre">input/jpg/</span></code> folder;</p></li>
<li><p>it runs the whole Caffe flow by calling <a class="reference external" href="files/caffe/caffe_flow_AlexNet.sh">caffe_flow_AlexNet.sh</a> script;</p></li>
<li><p>it launches the Vitis AI Quantizer  <a class="reference external" href="files/deploy/alexnetBNnoLRN/quantiz/vaiq_alexnetBNnoLRN.sh">vaiq_alexnetBNnoLRN.sh</a> and Compiler <a class="reference external" href="files/deploy/alexnetBNnoLRN/quantiz/vaic_alexnetBNnoLRN.sh">vaic_alexnetBNnoLRN.sh</a> scripts.</p></li>
</ul>
<p>In order of execution, the Python scripts that compose the Caffe design flow are listed below. They enable you to create the datasets, train your CNN with a training and validation LMDB database, and finally make predictions on JPEG images. They all are launched by <a class="reference external" href="files/caffe/caffe_flow_AlexNet.sh">caffe_flow_AlexNet.sh</a>.</p>
<p>a) <a class="reference external" href="files/caffe/code/1_write_cats-vs-dogs_images.py">1_write_cats-vs-dogs_images.py</a>: this script creates  subfolders <code class="docutils literal notranslate"><span class="pre">test</span></code>, <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">val</span></code> and  <code class="docutils literal notranslate"><span class="pre">calib</span></code>. The <code class="docutils literal notranslate"><span class="pre">calib</span></code> folder is needed only for quantization. You only need to execute it once.</p>
<p>b) <a class="reference external" href="files/caffe/code/2a_compute_mean.py">2a_compute_mean.py</a>: this script computes the three mean values for the <code class="docutils literal notranslate"><span class="pre">train_lmdb</span></code> database. You only need to execute it once.</p>
<p>c) <a class="reference external" href="files/caffe/code/2b_create_lmdb.py">2b_create_lmdb.py</a>: this script creates the LMDB databases <code class="docutils literal notranslate"><span class="pre">input/lmdb/train_lmdb</span></code> and <code class="docutils literal notranslate"><span class="pre">input/lmdb/valid_lmdb</span></code> for the training step. You only need to execute it once.</p>
<p>d) <a class="reference external" href="files/caffe/code/3_read_lmdb.py">3_read_lmdb.py</a>: this script can be used to debug the first two scripts.</p>
<p>e) <a class="reference external" href="files/caffe/code/4_training.py">4_training.py</a>: this script launches the real training process in Caffe, given certain <code class="docutils literal notranslate"><span class="pre">solver</span></code> and CNN description <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> files. To be used for any trial of training.</p>
<p>f) <a class="reference external" href="files/caffe/code/5_plot_learning_curve.py">5_plot_learning_curve.py</a>: this script is launched at the end of the training to plot the learning curves of accuracy and loss.</p>
<p>g) <a class="reference external" href="files/caffe/code/6_make_predictions.py">6_make_predictions.py</a>: this script is launched at the end of the training to measure the average prediction accuracy achieved by the CNN.</p>
</div>
<div class="section" id="alexnet-caffe-training-files">
<h1>7 AlexNet Caffe Training Files<a class="headerlink" href="#alexnet-caffe-training-files" title="Permalink to this headline">¶</a></h1>
<div class="section" id="cnn-model-description">
<h2>7.1 CNN Model Description<a class="headerlink" href="#cnn-model-description" title="Permalink to this headline">¶</a></h2>
<p>To describe the CNN in Caffe, you need a <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> text file which shows the type of layers and how they are connected, plus some specific actions to be done only during the training or validation phases, indicated as TRAIN and TEST respectively. You also need to set the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> during the TRAIN and TEST phases: 128 (or also 256, depending on the memory of your GPU card) and 80 respectively. During the TRAIN phase, all the parameters of the CNN are updated by the <code class="docutils literal notranslate"><span class="pre">Adam</span></code> solver, every <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> number of images.</p>
<p>The model giving the best top-1 prediction results in previous experiments is <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a>. Associated with it, you also have the <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/deploy_2_alexnetBNnoLRN.prototxt">deploy_2_alexnetBNnoLRN.prototxt</a>, which is needed to compute the prediction accuracy on the 1000 images in the <code class="docutils literal notranslate"><span class="pre">test</span></code> subfolder (the same that will be used at run time on the ZCU102).</p>
<p>There is another model, <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/q_train_val_2_alexnetBNnoLRN.prototxt">q_train_val_2_AalexnetBNnoLRN.prototxt</a>, which will be applied later during the quantization process of the baseline CNN. It is exactly the same as <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a>, but the LMDB database of the TRAIN phase has been replaced by the calibration images.</p>
</div>
<div class="section" id="caffe-training-process-solver">
<h2>7.2 Caffe Training Process Solver<a class="headerlink" href="#caffe-training-process-solver" title="Permalink to this headline">¶</a></h2>
<p>In Caffe, the solver file defines the optimization method (that is, <code class="docutils literal notranslate"><span class="pre">SGD</span></code>, or <code class="docutils literal notranslate"><span class="pre">Adam</span></code>, or <code class="docutils literal notranslate"><span class="pre">Nesterov</span></code>), the number of iterations, and the policy for changing the learning rate during the various iterations. It also says if a CPU or GPU is being used for computation.</p>
<p>The solver file is named <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/solver_2_alexnetBNnoLRN.prototxt">solver_2_alexnetBNnoLRN.prototxt</a>, and contains the settings for the training of the AlexNet model that have proved to be optimal during the previous experiments.</p>
</div>
<div class="section" id="prediction-accuracy-performance">
<h2>7.3 Prediction Accuracy Performance<a class="headerlink" href="#prediction-accuracy-performance" title="Permalink to this headline">¶</a></h2>
<p>The output of each process running on the host side is captured and stored into a single unified log file, named <a class="reference external" href="files/log/logfile_baseline_alexnet_host.txt">logfile_baseline_alexnet_host.txt</a>; in the following of this section some fragments of such file are reported.</p>
<p>After training is executed, the CNN has a top-1 average accuracy of 94% (with 20000 iterations) computed on the validation dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span> <span class="mi">2541</span> <span class="n">solver</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">384</span><span class="p">]</span> <span class="n">Iteration</span> <span class="mi">20000</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0335872</span>
<span class="o">...</span> <span class="mi">2541</span> <span class="n">solver</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">424</span><span class="p">]</span> <span class="n">Iteration</span> <span class="mi">20000</span><span class="p">,</span> <span class="n">Testing</span> <span class="n">net</span> <span class="p">(</span><span class="c1">#0)</span>
<span class="o">...</span> <span class="mi">2541</span> <span class="n">solver</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">523</span><span class="p">]</span> <span class="n">Test</span> <span class="n">net</span> <span class="n">output</span> <span class="c1">#0: accuracy = 0.94725</span>
<span class="o">...</span> <span class="mi">2541</span> <span class="n">solver</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">523</span><span class="p">]</span> <span class="n">Test</span> <span class="n">net</span> <span class="n">output</span> <span class="c1">#1: loss = 0.199691 (* 1 = 0.199691 loss)</span>
<span class="o">...</span> <span class="mi">2541</span> <span class="n">solver</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">523</span><span class="p">]</span> <span class="n">Test</span> <span class="n">net</span> <span class="n">output</span> <span class="c1">#2: top-1 = 0.94725</span>
</pre></div>
</div>
<p>When making predictions with the 1000 test images, the average top-1 prediction accuracy is still 94%:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>                <span class="n">precision</span> <span class="n">recall</span>  <span class="n">f1</span><span class="o">-</span><span class="n">score</span>   <span class="n">support</span>

<span class="n">cat</span>             <span class="mf">0.95</span>      <span class="mf">0.92</span>      <span class="mf">0.94</span>       <span class="mi">500</span>
<span class="n">dog</span>             <span class="mf">0.92</span>      <span class="mf">0.96</span>      <span class="mf">0.94</span>       <span class="mi">500</span>

<span class="n">accuracy</span>                            <span class="mf">0.94</span>      <span class="mi">1000</span>
<span class="n">macro</span> <span class="n">avg</span>       <span class="mf">0.94</span>      <span class="mf">0.94</span>      <span class="mf">0.94</span>      <span class="mi">1000</span>
<span class="n">weighted</span> <span class="n">avg</span>    <span class="mf">0.94</span>      <span class="mf">0.94</span>      <span class="mf">0.94</span>      <span class="mi">1000</span>

<span class="n">SKLEARN</span> <span class="n">Accuracy</span> <span class="o">=</span> <span class="mf">0.94</span>
</pre></div>
</div>
<p>Since  GPUs have varying random states, and you might not achieve exactly the same numerical results.</p>
</div>
</div>
<div class="section" id="quantization-of-alexnet">
<h1>8 Quantization of AlexNet<a class="headerlink" href="#quantization-of-alexnet" title="Permalink to this headline">¶</a></h1>
<div class="section" id="input-and-output-prototxt-files">
<h2>8.1 Input and Output .prototxt Files<a class="headerlink" href="#input-and-output-prototxt-files" title="Permalink to this headline">¶</a></h2>
<p>The Vitis AI Quantizer tool needs the following input files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">float.prototxt</span></code>: this is the description text file of the floating point CNN model;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code>: this is the  weights file of the CNN in floating point;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calibration</span> <span class="pre">dataset</span></code>: this is a subset of the images used in the original training, containing about 1000 pictures in this case study.</p></li>
</ul>
<p>When the quantization is done, two output files are generated. These become the inputs to the  Vitis AI Compiler:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">deploy.prototxt</span></code>: this is the new description text file of the quantized CNN model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deploy.caffemodel</span></code>: this is the file with fixed point quantized weights (note that this is not a standard Caffe format).</p></li>
</ul>
<p>Preparing the input <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> files requires the following steps.</p>
<p>a) Take the weights file generated after the Caffe training process (<code class="docutils literal notranslate"><span class="pre">snapshot_2_alexnetBNnoLRN__iter_20000.caffemodel</span></code>), and rename it simply <code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code>.</p>
<p>b) Take the description file used in the Caffe training process (<a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a>), and rename it simply <code class="docutils literal notranslate"><span class="pre">float.prototxt</span></code>.</p>
<p>c) Make the following further changes to the <code class="docutils literal notranslate"><span class="pre">float.prototxt</span></code> file:</p>
<ul class="simple">
<li><p>remove the <code class="docutils literal notranslate"><span class="pre">Datatype</span></code> layers for the original TRAIN phase;</p></li>
<li><p>add an <code class="docutils literal notranslate"><span class="pre">ImageData</span></code> type layer with the calibration images folder for the new TRAIN phase;</p></li>
<li><p>on the bottom, add one <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code> layer to compute top-1 prediction accuracy.</p></li>
</ul>
<p>For your reference, the above changes were already made in  <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/q_train_val_2_alexnetBNnoLRN.prototxt">q_train_val_2_alexnetBNnoLRN.prototxt</a> file.</p>
</div>
<div class="section" id="quantization-flow">
<h2>8.2 Quantization Flow<a class="headerlink" href="#quantization-flow" title="Permalink to this headline">¶</a></h2>
<p>The estimated top-1 average accuracy after quantization (computed on the validation dataset) can be seen in one of the last lines of the captured <a class="reference external" href="files/log/logfile_baseline_alexnet_host.txt">log file</a>, 91.8% is achieved (with only a 0.4% drop in comparison with the floating point model):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>... ] Test iter: 50/50, loss = 0.46556
... ] Test iter: 50/50, top-1 = 0.92
... ] Test Results:
... ] Loss: 0.201669
... ] accuracy = 0.9464
... ] loss = 0.201669 (* 1 = 0.201669 loss)
... ] top-1 = 0.9464
... ] Test Done!
</pre></div>
</div>
<p>The effective quantization flow is composed mainly of five steps.</p>
<ol class="simple">
<li><p>Quantize the CNN 32-bit floating point model to INT8 by using the Vitis AI Quantizer (see script <a class="reference external" href="files/deploy/alexnetBNnoLRN/quantiz/vaiq_alexnetBNnoLRN.sh">vaiq_alexnetBNnoLRN.sh</a>).</p></li>
<li><p>Compile the INT8 CNN model by using the Vitis AI Compiler (see script <a class="reference external" href="files/deploy/alexnetBNnoLRN/quantiz/vaic_alexnetBNnoLRN.sh">vaic_alexnetBNnoLRN.sh</a>) to generate the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file for the target board.</p></li>
<li><p>Use either VART C++ or Python APIs to write the hybrid application.  The term “hybrid” is adopted because the ARM CPU is executing some software routines while the DPU hardware accelerator is running the FC, CONV, ReLU, and BN layers of the CNN that were coded in the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code>file. See respectively the <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/src/main.cc">main.cc</a> and the <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/src/classification.py">classification.py</a> program application files.</p></li>
<li><p>Cross-compile the hybrid (CPU + DPU) application with <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/build_app.sh">build_app.sh</a> shell script on your <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> host environment. In reality this step is just optional, as the application will be compiled directly in the target board environment in next step 5.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">unset</span> LD_LIBRARY_PATH    
sh ~/petalinux_sdk/environment-setup-aarch64-xilinx-linux
<span class="nb">cd</span> &lt;WRK_DIR&gt;/tutorials/VAI-Caffe-ML-CATSvsDOGS/files
<span class="nb">cd</span> deploy/zcu102
bash -x ./build_app.sh
<span class="nb">cd</span> ..
tar -cvf zcu102.tar ./zcu102 <span class="c1"># to be copied on the SD card</span>
</pre></div>
</div>
<ol>
<li><p>Assuming you have transferred the <code class="docutils literal notranslate"><span class="pre">zcu102.tar</span></code> archive from the host to the target board with <code class="docutils literal notranslate"><span class="pre">scp</span></code> utility, now you can run the hybrid application (from the <em>target</em> board side). See below the command line example for the AlexNet case:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tar</span> <span class="o">-</span><span class="n">xvf</span> <span class="n">zcu102</span><span class="o">.</span><span class="n">tar</span>
<span class="n">cd</span> <span class="n">zcu102</span>
<span class="n">bash</span> <span class="o">./</span><span class="n">run_all_target</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>This command  also runs the script <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/src/check_dpu_runtime_accuracy.py">check_dpu_runtime_accuracy.py</a> to check the top-1 prediction accuracy that AI Inference DPU achieves at run time on the test images. This is the most important step, because you can now see the real average accuracy of your CNN system working at run time.</p>
</li>
</ol>
</div>
<div class="section" id="main-application">
<h2>8.3  Main  Application<a class="headerlink" href="#main-application" title="Permalink to this headline">¶</a></h2>
<p>This is the file that has to be compiled on the ARM CPU. It controls the DPU hardware accelerator using the VART APIs. <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/src/main.cc">main.cc</a> computes the top-2 prediction accuracy. If you change the way this information is printed in the <code class="docutils literal notranslate"><span class="pre">stdout</span></code>, you must also change the Python script <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/src/check_dpu_runtime_accuracy.py">check_dpu_runtime_accuracy.py</a> accordingly, because it acts essentially as a text parser of the <code class="docutils literal notranslate"><span class="pre">logfile_target_baseline.txt</span></code> captured at run time.</p>
</div>
<div class="section" id="performance-of-the-quantized-baseline-alexnet-on-zcu102">
<h2>8.4 Performance of the Quantized Baseline AlexNet on ZCU102<a class="headerlink" href="#performance-of-the-quantized-baseline-alexnet-on-zcu102" title="Permalink to this headline">¶</a></h2>
<p>At the end of this quantization procedure, when the AI Inference DPU runs the <code class="docutils literal notranslate"><span class="pre">alexnetBNnoLRN</span></code> on the ZCU102 to make predictions on the 1000 test images, the following performance is reported as illustrated by the following fragments of <a class="reference external" href="files/log/logfile_summary_target.txt">logfile_summary_target.txt</a>:</p>
<ul class="simple">
<li><p>129 fps with five threads</p></li>
<li><p>94% top-1 average accuracy</p></li>
</ul>
</div>
</div>
<div class="section" id="pruning-alexnet">
<h1>9 Pruning  AlexNet<a class="headerlink" href="#pruning-alexnet" title="Permalink to this headline">¶</a></h1>
<p>The pruning reference documentation can be found in the <a class="reference external" href="https://www.xilinx.com/html_docs/vitis_ai/1_0/thf1576862844211.html">Vitis AI Optimizer UG1431</a>. This section gives instructions to effectively prune the AlexNet CNN.</p>
<div class="section" id="id1">
<h2>9.1 Introduction<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Pruning is a technique to remove redundant or less useful weights and output channels from a CNN layer to reduce or compress the overall number of operations. The aim is to reduce the number of operations and increase the frames per second; you might not need it if your CNN is already optimized by design. This can however be detrimental to the average top-1 accuracy: the final result is ultimately a trade-off between the desired compression and the effective accuracy to sustain a certain target frame rate.</p>
<p>There are usually two types of pruning: fine and coarse. Fine pruning selectively kills either the weights or the output features with the smallest values from a channel. To achieve higher frame rates from a fine-compressed CNN, the hardware accelerator must be enabled to perform zero-skipping (that is, skipping all the multiplications with zero values). Zero-skipping requires a proper hardware architecture and organization of non-zero data (usually with <a class="reference external" href="https://en.wikipedia.org/wiki/Run-length_encoding">run-length coding</a>) in the internal memory of the hardware accelerator; otherwise, there would be no performance gain from fine pruning.</p>
<p>The Xilinx Vitis AI Optimizer applies coarse pruning, which involves removing a complete output channel. In this case, any hardware accelerator can gain from it.</p>
<p>However, this invasive kind of pruning can affect the average accuracy. It is therefore important to apply the pruning in an iterative manner: for example, by compressing the CNN by only 10% and then performing fine-tuning (which can be a complete training process) to recover the probable accuracy drop. If you work carefully and apply this process for 7-8 steps (or even more), you can arrive at 70-80% of compression with a negligible top-1 average accuracy decrease. This iterative process can take a lot of time, especially if you are using a large database.</p>
<p>At the end of the pruning process, you get a new  floating point <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code> file of a size probably reduced by 40-60% (depending on the CNN) in comparison with the original <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code> file of the baseline (non-pruned) CNN. To run it on the ZCU102 board, you need to apply quantization using the output files generated by pruning (with some minor but important manual editing) as the input file to quantization.</p>
</div>
<div class="section" id="prepare-the-input-files-for-pruning">
<h2>9.2 Prepare the Input Files for Pruning<a class="headerlink" href="#prepare-the-input-files-for-pruning" title="Permalink to this headline">¶</a></h2>
<p>Before you begin, you need to have the following files in the <code class="docutils literal notranslate"><span class="pre">files/pruning/alexnetBNnoLRN/</span></code> working directory:</p>
<ul class="simple">
<li><p><a class="reference external" href="files/pruning/alexnetBNnoLRN/config.prototxt">config.prototxt</a>: Use this file to set the number of GPU devices and test iterations, as well as the Caffe model description, weights files, and compression ratio you want to achieve. In reality, seven files like this are indeed adopted, and each one applies the weights generated by the previous pruning trial to increment the compression by 10%.</p></li>
<li><p><a class="reference external" href="files/pruning/alexnetBNnoLRN/solver.prototxt">solver.prototxt</a>: This is the same solver of your original <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code>, just renamed (for example, the same <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/solver_2_alexnetBNnoLRN.prototxt">solver_2_alexnetBNnoLRN.prototxt</a> that was already adopted during the training process). In general, you can also try to reduce the amount of iterations, depending on your specific case. For this CNN, 12000 iterations per each 10% of pruning step are good enough, instead of using the 20000 iterations as in the original training.</p></li>
<li><p><a class="reference external" href="files/pruning/alexnetBNnoLRN/train_val.prototxt">train_val.prototxt</a>: This is the same description file of your original <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code>, but renamed. For example, it is the same as the <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a>. Note that you need to edit <a class="reference external" href="files/pruning/alexnetBNnoLRN/train_val.prototxt">train_val.prototxt</a> to add top-1 layer at its end.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code>. This is the same weights file of your original <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code>, only renamed (for example, the same <code class="docutils literal notranslate"><span class="pre">snapshot_2_alexnetBNnoLRN__iter_20000.caffemodel</span></code>).</p></li>
</ul>
</div>
<div class="section" id="pruning-flow">
<h2>9.3 Pruning Flow<a class="headerlink" href="#pruning-flow" title="Permalink to this headline">¶</a></h2>
<p>In previous experiments, pruning the AlexNet required seven steps of 10% compression each time. The flow can be explained by looking at the <a class="reference external" href="files/pruning/alexnetBNnoLRN/pruning_flow.sh">pruning_flow.sh</a> shell script:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">analysis</span> <span class="pre">(ana)</span></code> has to be executed only once at the beginning, and generates a hidden text file, <code class="docutils literal notranslate"><span class="pre">.ana.regular</span></code>, that is then reused by all following trials. This process can take a lot of time, so it is recommended to comment out the related line in the shell script after you have executed it once (assuming you are not changing the input files).</p></li>
<li><p>Seven steps  of <code class="docutils literal notranslate"><span class="pre">compress</span> <span class="pre">/</span> <span class="pre">finetune</span></code> actions, each one compressing the previously compressed CNN by 10% more. In particular, <code class="docutils literal notranslate"><span class="pre">compress</span></code> is responsible for heuristically selecting the channels to kill, while <code class="docutils literal notranslate"><span class="pre">finetune</span></code> performs a retrain to restore the top-1 accuracy at the previous value if possible.</p></li>
<li><p>The final action is <code class="docutils literal notranslate"><span class="pre">transform</span></code>, which transforms the intermediate sparse CNN model into the effective output <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code> of the compressed CNN (<code class="docutils literal notranslate"><span class="pre">transform.caffemodel</span></code>).</p></li>
</ul>
<p>In the <code class="docutils literal notranslate"><span class="pre">transform</span></code> step, you need to complete the following steps:</p>
<ol class="simple">
<li><p>Take the <a class="reference external" href="files/pruning/alexnetBNnoLRN/regular_rate_0.7/final.prototxt">final.prototxt</a> generated by the seventh step of <code class="docutils literal notranslate"><span class="pre">compress-finetune</span></code>.</p></li>
<li><p>Take the final finetuned <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code> named <code class="docutils literal notranslate"><span class="pre">regular_rate_0.7/snapshot/_iter_12000.caffemodel</span></code>. This is also illustrated in the <a class="reference external" href="files/pruning/alexnetBNnoLRN/pruning_flow.sh">pruning_flow.sh</a> shell script.</p></li>
</ol>
<p>The command to prune the whole CNN is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">WRK_DIR</span><span class="o">&gt;/</span><span class="n">tutorials</span><span class="o">/</span><span class="n">VAI</span><span class="o">-</span><span class="n">Caffe</span><span class="o">-</span><span class="n">ML</span><span class="o">-</span><span class="n">CATSvsDOGS</span><span class="o">/</span><span class="n">files</span><span class="o">/</span>
<span class="n">bash</span> <span class="n">pruning</span><span class="o">/</span><span class="n">alexnetBNnoLRN</span><span class="o">/</span><span class="n">pruning_flow</span><span class="o">.</span><span class="n">sh</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> <span class="o">|</span> <span class="n">tee</span> <span class="n">logfile_pruned_alexnet_host</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p><strong>:warning: IMPORTANT:</strong> Check or change the pathnames inside the <code class="docutils literal notranslate"><span class="pre">solver</span></code>, <code class="docutils literal notranslate"><span class="pre">train_val</span></code>, and <code class="docutils literal notranslate"><span class="pre">config*.prototxt</span></code> files mentioned above.</p>
</div>
<div class="section" id="pruning-results">
<h2>9.4 Pruning Results<a class="headerlink" href="#pruning-results" title="Permalink to this headline">¶</a></h2>
<p>After seven rounds of <code class="docutils literal notranslate"><span class="pre">compress</span></code> and <code class="docutils literal notranslate"><span class="pre">finetune</span></code> two output files are generated from the three input files (<code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code>,  <a class="reference external" href="files/pruning/alexnetBNnoLRN/solver.prototxt">solver.prototxt</a> and <a class="reference external" href="files/pruning/alexnetBNnoLRN/train_val.prototxt">train_val.prototxt</a>). The output files are <code class="docutils literal notranslate"><span class="pre">transformed.caffemodel</span></code> and  <a class="reference external" href="files/pruning/alexnetBNnoLRN/regular_rate_0.7/final.prototxt">final.prototxt</a>. These become the input files to the next quantization process.</p>
<p>The compressed AlexNet now has ~70% less operations and ~88% less weights than the original baseline CNN, as reported in the <code class="docutils literal notranslate"><span class="pre">logfile_pruned_alexnet_host.txt</span></code>.</p>
<p>After the last <code class="docutils literal notranslate"><span class="pre">finetune</span></code> step, the estimated top-1 average prediction accuracy is  ~95%. In this case, the top-1 accuracy is measured on the validation dataset. To measure the effective top-1 average accuracy on the DPU at run time, you need to quantize the CNN you just pruned.</p>
<p>In conclusion, the original baseline floating point CNN model has the following complexity:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span> <span class="n">net_counter</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">114</span><span class="p">]</span> <span class="n">Total</span> <span class="n">operations</span><span class="p">:</span> <span class="mi">2153918368</span>
<span class="o">...</span> <span class="n">net_counter</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">115</span><span class="p">]</span> <span class="n">Total</span> <span class="n">params</span><span class="p">:</span>        <span class="mi">3764995</span>
</pre></div>
</div>
<p>whereas the pruned CNN has:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span> <span class="n">net_counter</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">84</span><span class="p">]</span> <span class="n">Total</span> <span class="n">operations</span><span class="p">:</span> <span class="mi">636272830</span>
<span class="o">...</span> <span class="n">net_counter</span><span class="o">.</span><span class="n">cpp</span><span class="p">:</span><span class="mi">85</span><span class="p">]</span> <span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">425467</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="quantization-of-pruned-alexnet">
<h1>10 Quantization of Pruned AlexNet<a class="headerlink" href="#quantization-of-pruned-alexnet" title="Permalink to this headline">¶</a></h1>
<div class="section" id="pruned-quantization-flow">
<h2>10.1 Pruned Quantization Flow<a class="headerlink" href="#pruned-quantization-flow" title="Permalink to this headline">¶</a></h2>
<p>The process is exactly the same at that explained in the <a class="reference external" href="README.md#8-quantization-of-the-alexnet">Quantization of the AlexNet</a> section. The only difference is that the input files are now named as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">transformed.caffemodel</span></code>: The output of the <code class="docutils literal notranslate"><span class="pre">transform</span></code> step from the pruning process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">q_final.prototxt</span></code>: Generated by manually editing the same <a class="reference external" href="files/pruning/alexnetBNnoLRN/regular_rate_0.7/final.prototxt">final.prototxt</a> that was produced at the end of the pruning process.</p></li>
</ul>
<p>You also need to replace the training LMDB database with the calibration images, and add the three mean values. Pruning is not a deterministic process, so every pruning trial can create a different <code class="docutils literal notranslate"><span class="pre">final.prototxt</span></code> file, and in that case you have to re-edit a new <code class="docutils literal notranslate"><span class="pre">q_final.prototxt</span></code> file (the <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/q_final.prototxt">q_final.prototxt</a> is solely for documentation).</p>
<p>To compile with the Vitis AI quantizer and compiler tools, call the two shell scripts, <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/vaiq_pruned_alexnetBNnoLRN.sh">vaiq_pruned_alexnetBNnoLRN.sh</a> and <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/vaic_pruned_alexnetBNnoLRN.sh">vaic_pruned_alexnetBNnoLRN.sh</a>, with the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">WRK_DIR</span><span class="o">/</span><span class="n">tutoriala</span><span class="o">/</span><span class="n">VAI</span><span class="o">-</span><span class="n">Caffe</span><span class="o">-</span><span class="n">ML</span><span class="o">-</span><span class="n">CATSvsDOGS</span><span class="o">/</span><span class="n">files</span><span class="o">/</span>
<span class="n">source</span> <span class="n">deploy</span><span class="o">/</span><span class="n">alexnetBNnoLRN</span><span class="o">/</span><span class="n">vaiq_pruned_alexnetBNnoLRN</span><span class="o">.</span><span class="n">sh</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> <span class="o">|</span> <span class="n">tee</span> <span class="n">deploy</span><span class="o">/</span><span class="n">alexnetBNnoLRN</span><span class="o">/</span><span class="n">pruned</span><span class="o">/</span><span class="n">rpt</span><span class="o">/</span><span class="n">logfile_vaiq_pruned_alexnetBNnoLRN</span><span class="o">.</span><span class="n">txt</span>
<span class="n">source</span> <span class="n">deploy</span><span class="o">/</span><span class="n">alexnetBNnoLRN</span><span class="o">/</span><span class="n">vaic_pruned_alexnetBNnoLRN</span><span class="o">.</span><span class="n">sh</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> <span class="o">|</span> <span class="n">tee</span> <span class="n">deploy</span><span class="o">/</span><span class="n">alexnetBNnoLRN</span><span class="o">/</span><span class="n">pruned</span><span class="o">/</span><span class="n">rpt</span><span class="o">/</span><span class="n">logfile_vaic_pruned_alexnetBNnoLRN</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>At the end of the Quantization process on the pruned AlexNet the top-1 accuracy is 0.9564 as illustrated in the following fragment of <a class="reference external" href="files/log/logfile_pruned_alexnet_host.txt">logfile_pruned_alexnet_host.txt</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>... ] Test iter: 50/50, loss = 0.129312
... ] Test iter: 50/50, top-1 = 0.98
... ] Test Results:
... ] Loss: 0.253436
... ] accuracy = 0.9564
... ] loss = 0.253436 (* 1 = 0.253436 loss)
... ] top-1 = 0.9564
... ] Test Done!
</pre></div>
</div>
<p>Now cross-compile the hybrid (CPU + DPU) application with <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/build_app.sh">build_app.sh</a> shell script.
Again, this step is optional (similarly to the baseline CNN case), as the compilation will be done directly on the target board.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">unset</span> LD_LIBRARY_PATH    
sh ~/petalinux_sdk/environment-setup-aarch64-xilinx-linux
<span class="nb">cd</span> &lt;WRK_DIR&gt;/tutorials/VAI-Caffe-ML-CATSvsDOGS/files
<span class="nb">cd</span> deploy/zcu102/code
bash ./build_app.sh
<span class="nb">cd</span> ..
tar -cvf zcu102.tar ./zcu102 <span class="c1"># to be copied on the SD card</span>
</pre></div>
</div>
<p>Assuming you have transferred the <code class="docutils literal notranslate"><span class="pre">zcu102.tar</span></code> archive from the host to the target board with <code class="docutils literal notranslate"><span class="pre">scp</span></code> utility, now you can run the hybrid application (from the target board side). See below the command line example for the AlexNet case:
<code class="docutils literal notranslate"><span class="pre">tar</span> <span class="pre">-xvf</span> <span class="pre">zcu102.tar</span> <span class="pre">cd</span> <span class="pre">zcu102</span> <span class="pre">source</span> <span class="pre">run_all_target.sh</span></code></p>
</div>
<div class="section" id="performance-of-the-quantized-pruned-alexnet-on-zcu102">
<h2>10.2 Performance of the Quantized Pruned AlexNet on ZCU102<a class="headerlink" href="#performance-of-the-quantized-pruned-alexnet-on-zcu102" title="Permalink to this headline">¶</a></h2>
<p>At the end of the pruning and quantization procedures, when the AI Inference DPU runs the AlexNet CNN on the ZCU102, the following performance was reported:</p>
<ul class="simple">
<li><p>367 fps with seven threads,</p></li>
<li><p>95% average top-1 accuracy,
as shown in the <a class="reference external" href="files/log/logfile_summary_target.txt">logfile_summary_target.txt</a> log file.</p></li>
</ul>
</div>
<div class="section" id="alexnet-summary">
<h2>11  AlexNet Summary<a class="headerlink" href="#alexnet-summary" title="Permalink to this headline">¶</a></h2>
<p>Congratulations! You have completed the Caffe training of AlexNet using the Dogs vs. Cats database. You then applied Xilinx Vitis AI tools to quantize the original CNN to get a baseline reference. You also have seen how to prune (and quantize again) the new optimized CNN: the CNN is optimized in the sense that it has fewer output channels than its baseline version.</p>
<p>By running the <code class="docutils literal notranslate"><span class="pre">.xmodel</span></code> files of either the baseline or the pruned CNNs on the ZCU102 target board, you have measured a frame rate improvement from 129 fps (baseline) to 367 fps (pruned) with an average top-1 accuracy of 94% and 95%  respectively for the baseline and pruned CNNs.</p>
<p>See below for a summary of the most important steps you have completed to arrive at this point.</p>
<ol class="simple">
<li><p>You have downloaded the Dogs vs. Cats dataset and organized it in the proper way to train AlexNet with Caffe, making predictions on 1000 testing images to get the average top-1 accuracy: 20000 and 4000 images respectively in the LMDB training and validation databases, plus 200 images (from the training dataset) to compose the calibration dataset to be used during the quantization process. You have applied the following Python scripts:</p></li>
</ol>
<ul class="simple">
<li><p><a class="reference external" href="files/caffe/code/1_write_cats-vs-dogs_images.py">1_write_cats-vs-dogs_images.py</a></p></li>
<li><p><a class="reference external" href="files/caffe/code/2a_compute_mean.py">2a_compute_mean.py</a></p></li>
<li><p><a class="reference external" href="files/caffe/code/2b_create_lmdb.py">2b_create_lmdb.py</a></p></li>
</ul>
<ol class="simple">
<li><p>You have trained the CNN with 20000 iterations by applying the <a class="reference external" href="files/caffe/code/4_training.py">4_training.py</a> Python script  and the  <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a> and <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/solver_2_alexnetBNnoLRN.prototxt">solver_2_alexnetBNnoLRN.prototxt</a> input <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> files. You have also plotted the learning curves of the training process with the <a class="reference external" href="files/caffe/code/5_plot_learning_curve.py">5_plot_learning_curve.py</a> Python script.</p></li>
<li><p>The floating point weights file <code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code> generated in the previous step together with the CNN deploy model (<a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/deploy_2_alexnetBNnoLRN.prototxt">deploy_2_alexnetBNnoLRN.prototxt</a>) have then been used to make predictions on the 1000 testing images. You have achieved an average top-1 accuracy of ~93%. All the above steps can be run with the single shell script <a class="reference external" href="files/caffe/caffe_flow_AlexNet.sh">caffe_flow_AlexNet.sh</a>.</p></li>
<li><p>You have then quantized this baseline CNN with  vitis AI quantizer and compiler tools on the host PC by applying the <a class="reference external" href="files/deploy/alexnetBNnoLRN/quantiz/vaiq_alexnetBNnoLRN.sh">vaiq_alexnetBNnoLRN.sh</a> and <a class="reference external" href="files/deploy/alexnetBNnoLRN/quantiz/vaic_alexnetBNnoLRN.sh">vaic_alexnetBNnoLRN.sh</a> shell scripts to the files generated in step 3, <code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code> and <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/q_train_val_2_alexnetBNnoLRN.prototxt">q_float.prototxt</a>, where the latter file is the <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a> edited to replace the LMDB training database with the calibration images and to add in the bottom the top-1 accuracy layer.</p></li>
<li><p>You have cross-compiled the hybrid application, composed of the <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/code/src/main.cc">main.cc</a> file and the <code class="docutils literal notranslate"><span class="pre">.xmodel</span></code> DPU kernel generated by Vitis AI compiler in the previous step.  Then you have copied everything to the target board and run it. The application is called “hybrid” because the ARM CPU is executing the SoftMax and top-2 software routines while the DPU hardware accelerator is running the FC, CONV, ReLU, and BN layers of the CNN.</p></li>
<li><p>You have measured an effective frame rate of 129fps and an average top-1 accuracy of 94% (this last one using the <a class="reference external" href="files/deploy/alexnetBNnoLRN/zcu102/baseline/check_dpu_runtime_accuracy.py">check_dpu_runtime_accuracy.py</a> Python script). This ends the implementation flow of the baseline <code class="docutils literal notranslate"><span class="pre">alexnetBNnoLRN</span></code> from the concept to the run-time execution on the ZCU102 target board.</p></li>
<li><p>You have seen how the CNN can be optimized by applying pruning to reduce the number of output channels, and consequently the overall number of operations the DPU has to complete. You have applied the iterative flow described in the <a class="reference external" href="files/pruning/alexnetBNnoLRN/pruning_flow.sh">pruning_flow.sh</a> shell script together with seven variances of the same <a class="reference external" href="files/pruning/alexnetBNnoLRN/config.prototxt">config.prototxt</a> configuration file to the following input files:</p></li>
</ol>
<ul class="simple">
<li><p><a class="reference external" href="files/pruning/alexnetBNnoLRN/solver.prototxt">solver.prototxt</a>: The same solver <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/solver_2_alexnetBNnoLRN.prototxt">solver_2_alexnetBNnoLRN.prototxt</a> adopted during the training process, with edited pathnames and 12000 iterations instead of 20000.</p></li>
<li><p><a class="reference external" href="files/pruning/alexnetBNnoLRN/train_val.prototxt">train_val.prototxt</a>: The same description file adopted during the training process, renamed from <a class="reference external" href="files/caffe/models/alexnetBNnoLRN/m2/train_val_2_alexnetBNnoLRN.prototxt">train_val_2_alexnetBNnoLRN.prototxt</a> with some editing to add top-1 accuracy layer at its end.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code>, the same weights file of your original <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code> (<code class="docutils literal notranslate"><span class="pre">snapshot_3_alexnetBNnoLRN__iter_20000.caffemodel</span></code>).</p></li>
</ul>
<ol class="simple">
<li><p>The pruning process generated the following output files, which then became inputs to the next and final quantization step:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">transformed.caffemodel</span></code>: A <code class="docutils literal notranslate"><span class="pre">.caffemodel</span></code> binary file much smaller in size than the starting <code class="docutils literal notranslate"><span class="pre">float.caffemodel</span></code>.</p></li>
<li><p><a class="reference external" href="files/pruning/alexnetBNnoLRN/regular_rate_0.7/final.prototxt">final.prototxt</a>: A <code class="docutils literal notranslate"><span class="pre">.prototxt</span></code> file detailing how many channels every layer has after pruning.</p></li>
</ul>
</li>
<li><p>You have edited the <a class="reference external" href="files/pruning/alexnetBNnoLRN/regular_rate_0.7/final.prototxt">final.prototxt</a> file to replace the LMDB training database with the calibration images, adding the top-1 accuracy layers in the bottom to get the new <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/q_final.prototxt">q_final.prototxt</a> file. You have applied the Vitis AI quantizer and compiler tools on the <em>host</em> PC, by applying the <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/vaiq_pruned_alexnetBNnoLRN.sh">vaiq_pruned_alexnetBNnoLRN.sh</a> and <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/vaic_pruned_alexnetBNnoLRN.sh">vaic_pruned_alexnetBNnoLRN.sh</a> shell scripts to the  <a class="reference external" href="files/deploy/alexnetBNnoLRN/pruned/q_final.prototxt">q_final.prototxt</a> and <code class="docutils literal notranslate"><span class="pre">transformed.caffemodel</span></code> files.</p></li>
<li><p>As in step 5, you have cross-compiled the hybrid application and then you copy it on the target ZCU102 board and run it there. You have measured a frame rate of 367 fps with an average top-1 accuracy of 95%.</p></li>
</ol>
</div>
</div>
<div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/BVLC/caffe">BVLC Caffe GitHub page</a></p></li>
<li><p><a class="reference external" href="https://www.researchgate.net/publication/308895193_Designing_Deep_Learning_Neural_Networks_using_Caffe">Designing Deep Learning Neural Networks using Caffe</a></p></li>
<li><p><a class="reference external" href="https://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html">Deep learning tutorial on Caffe technology : basic commands, Python and C++ code</a></p></li>
<li><p><a class="reference external" href="https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.5.3/navigation/pai_getstarted_caffe.html">IBM Getting Started with Caffe</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/html_docs/vitis_ai/1_0/vdy1575039492108.html">Xilinx Vitis AI Caffe Examples</a></p></li>
<li><p><a class="reference external" href="http://on-demand.gputechconf.com/gtc/2015/webinar/deep-learning-course/getting-started-with-caffe.pdf">Getting Started with Caffe (PDF)</a></p></li>
<li><p><a class="reference external" href="http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/">A Practical Introduction to Deep Learning with Caffe and Python</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;alexrachnog/using-caffe-with-your-own-dataset-b0ade5d71233">Using Caffe with your own dataset</a></p></li>
<li><p><a class="reference external" href="http://graphics.cs.cmu.edu/courses/16-824/2016_spring/slides/caffe_tutorial.pdf">Caffe Tutorial PDF Slides</a></p></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
															</div>
														</div>
														<div class="col-md-pull-6 col-lg-pull-6 col-md-6 col-lg-6">
															<span class="copyright">
                                  
                                  &copy; 2020–2021, Xilinx, Inc.
                              </span>
															<ul class="list-inline sub-menu">
																<li>
																	<a href="https://www.xilinx.com/about/privacy-policy.html">Privacy</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/legal.html">Legal</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/contact.html">Contact</a>
																</li>
															</ul>
														</div>
													</div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="quicklinks parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<noindex>
						<span class="quickLinks">
							<ul>
								<li>
									<a href="#top" class="btn backToTop">
									<span class="fas fa-angle-up" aria-hidden="true"></span>
									</a>
								</li>
							</ul>
						</span>
					</noindex>
				</div>
			</div>
		</div>
		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>